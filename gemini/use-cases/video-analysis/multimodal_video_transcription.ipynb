{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtxoQixAqoNu"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke8cM4GQln_c"
      },
      "source": [
        "# Unlocking Multimodal Video Transcription with Gemini\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPRBl_orqoNv"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/video-analysis/multimodal_video_transcription.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fvideo-analysis%2Fmultimodal_video_transcription.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/video-analysis/multimodal_video_transcription.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/video-analysis/multimodal_video_transcription.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<p><div style=\"clear: both;\"></div></p>\n",
        "\n",
        "<p>\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/video-analysis/multimodal_video_transcription.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/video-analysis/multimodal_video_transcription.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/video-analysis/multimodal_video_transcription.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/video-analysis/multimodal_video_transcription.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/video-analysis/multimodal_video_transcription.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0AcsmQ5hl9a"
      },
      "source": [
        "| Author                                           |\n",
        "| ------------------------------------------------ |\n",
        "| [Laurent Picard](https://github.com/PicardParis) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Hl-l4rhdvV"
      },
      "source": [
        "---\n",
        "\n",
        "## âœ¨ Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoN_-ofRhl9b"
      },
      "source": [
        "![intro image](https://storage.googleapis.com/github-repo/generative-ai/gemini/use-cases/video-analysis/multimodal_video_transcription/unlocking-multimodal-video-transcription.gif)\n",
        "\n",
        "Traditional machine learning (ML) perception models typically focus on specific features and single modalities, deriving insights solely from natural language, speech, or vision analysis. Historically, extracting and consolidating information from multiple modalities has been challenging due to siloed processing, complex architectures, and the risk of data being \"lost in translation.\" However, multimodal and long-context large language models (LLMs) like Gemini can overcome these issues by processing all modalities within the same context, opening new possibilities.\n",
        "\n",
        "Moving beyond speech-to-text, this notebook explores how to achieve comprehensive video transcription by leveraging all available modalities. It covers the following topics:\n",
        "\n",
        "- A methodology for addressing new or complex problems with a multimodal LLM\n",
        "- A prompt technique for decoupling data and preserving attention: tabular extraction\n",
        "- Strategies for making the most of Gemini's 1M-token context in a single request\n",
        "- Practical examples of multimodal video transcriptions\n",
        "- Tips & optimizations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwKb-__qK02C"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ”¥ Challenge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35E-CpC6qoNw"
      },
      "source": [
        "To fully transcribe a video, we're looking to answer the following questions:\n",
        "\n",
        "- 1ï¸âƒ£ What was said and when?\n",
        "- 2ï¸âƒ£ Who are the speakers?\n",
        "- 3ï¸âƒ£ Who said what?\n",
        "\n",
        "Can we solve this problem in a straightforward and efficient way?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oArL-WR6qoNx"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸŒŸ State of the art\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEqIf_8wqoNx"
      },
      "source": [
        "### 1ï¸âƒ£ What was said and when?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8QHpIophj3y"
      },
      "source": [
        "This is a known problem with an existing solution:\n",
        "\n",
        "- **Speech-to-Text** (STT) is a process that takes an audio input and transforms speech into text. STT can provide timestamps at the word level. It is also known as automatic speech recognition (ASR).\n",
        "\n",
        "In the last decade, task-specific ML models have most effectively addressed this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASAjs9XIhj3y"
      },
      "source": [
        "### 2ï¸âƒ£ Who are the speakers?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPVOzTOkhj3y"
      },
      "source": [
        "We can retrieve speaker names in a video from two sources:\n",
        "\n",
        "- **What's written** (e.g., speakers can be introduced with on-screen information when they first speak)\n",
        "- **What's spoken** (e.g., \"Hello Bob! Alice! How are you doing?\")\n",
        "\n",
        "Vision and Natural Language Processing (NLP) models can help with the following features:\n",
        "\n",
        "- Vision: **Optical Character Recognition** (OCR), also called text detection, extracts the text visible in images.\n",
        "- Vision: **Person Detection** identifies if and where people are in an image.\n",
        "- NLP: **Entity Extraction** can identify named entities in text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XXLvBn7hj3y"
      },
      "source": [
        "### 3ï¸âƒ£ Who said what?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTCIysHbhj3z"
      },
      "source": [
        "This is another known problem with a partial solution (complementary to Speech-to-Text):\n",
        "\n",
        "- **Speaker Diarization** (also known as speaker turn segmentation) is a process that splits an audio stream into segments for the different detected speakers (\"Speaker A\", \"Speaker B\", etc.).\n",
        "\n",
        "Researchers have made significant progress in this field for decades, particularly with ML models in recent years, but this is still an active field of research. Existing solutions have shortcomings, such as requiring human supervision and hints (e.g., the minimum and maximum number of speakers, the language spoken), and supporting a limited set of languages.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FaQEMUFln_e"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸº Traditional ML pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T23-7OyFln_e"
      },
      "source": [
        "Solving all of 1ï¸âƒ£, 2ï¸âƒ£, and 3ï¸âƒ£ isn't straightforward. This would likely involve setting up an elaborate supervised processing pipeline, based on a few state-of-the-art ML models, such as the following:\n",
        "\n",
        "![a traditional ml pipeline](https://storage.googleapis.com/github-repo/generative-ai/gemini/use-cases/video-analysis/multimodal_video_transcription/traditional-ml-pipeline.png)\n",
        "\n",
        "We might need days or weeks to design and set up such a pipeline. Additionally, at the time of writing, our multimodal-video-transcription challenge is not a solved problem, so there's absolutely no certainty of reaching a viable solution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5D5Mrt9qoNx"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ’¡ A new problem-solving toolbox\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3keeSV5f1EAc"
      },
      "source": [
        "Gemini allows for rapid prompt-based problem solving. With just text instructions, we can extract information and transform it into new insights, through a straightforward and automated workflow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cebBAgoMhj3z"
      },
      "source": [
        "### ðŸŽ¬ Multimodal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdKTFUhWhj3z"
      },
      "source": [
        "Gemini is natively multimodal, which means it can process different types of inputs:\n",
        "\n",
        "- text\n",
        "- image\n",
        "- audio\n",
        "- video\n",
        "- document\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61K-rN1Whj3z"
      },
      "source": [
        "### ðŸŒ Multilingual\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy3C66nwhj3z"
      },
      "source": [
        "Gemini is also [multilingual](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#languages-gemini):\n",
        "\n",
        "- It can process inputs and generate outputs in 100+ languages\n",
        "- If we can solve the video challenge for one language, that solution should naturally extend to all other languages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcyl7KUrhj3z"
      },
      "source": [
        "### ðŸ§° A natural-language toolbox\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch5tuuaNhj3z"
      },
      "source": [
        "Multimodal and multilingual understanding in a single model lets us shift from relying on task-specific ML models to using a single versatile LLM.\n",
        "\n",
        "Our challenge now looks a lot simpler:\n",
        "\n",
        "![natural-language toolbox with gemini](https://storage.googleapis.com/github-repo/generative-ai/gemini/use-cases/video-analysis/multimodal_video_transcription/gemini-natural-language-toolbox.png)\n",
        "\n",
        "In other words, let's rephrase our challenge: Can we fully transcribe a video with just the following?\n",
        "\n",
        "- 1 video\n",
        "- 1 prompt\n",
        "- 1 request\n",
        "\n",
        "Let's try with Geminiâ€¦\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0_VsUthqoNx"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wbID7ORqoNx"
      },
      "source": [
        "### ðŸ Python packages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZCjtEXjhj3z"
      },
      "source": [
        "We'll use the following packages:\n",
        "\n",
        "- `google-genai`: the [Google Gen AI Python SDK](https://pypi.org/project/google-genai) lets us call Gemini with a few lines of code\n",
        "- `pandas` for data visualization\n",
        "\n",
        "We'll also use these packages (dependencies of `google-genai`):\n",
        "\n",
        "- `pydantic` for data management\n",
        "- `tenacity` for request management\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kZBN80r7qtgs"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet \"google-genai>=1.31.0\" \"pandas[output-formatting]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGgmHVdQqoNz"
      },
      "source": [
        "### ðŸ”— Gemini API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOJv5DjZhj3z"
      },
      "source": [
        "We have two main options to send requests to Gemini:\n",
        "\n",
        "- [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs): Build enterprise-ready projects on Google Cloud\n",
        "- [Google AI Studio](https://aistudio.google.com): Experiment, prototype, and deploy small projects\n",
        "\n",
        "The Google Gen AI SDK provides a unified interface to these APIs and we can use environment variables for the configuration.\n",
        "\n",
        "**Option A - Gemini API via Vertex AI**\n",
        "\n",
        "Requirement:\n",
        "\n",
        "- A Google Cloud project\n",
        "- The [Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com) must be enabled for this project\n",
        "\n",
        "Gen AI SDK environment variables:\n",
        "\n",
        "- `GOOGLE_GENAI_USE_VERTEXAI=\"True\"`\n",
        "- `GOOGLE_CLOUD_PROJECT=\"<PROJECT_ID>\"`\n",
        "- `GOOGLE_CLOUD_LOCATION=\"<LOCATION>\"` (see [Google model endpoint locations](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations#google_model_endpoint_locations))\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n",
        "\n",
        "**Option B - Gemini API via Google AI Studio**\n",
        "\n",
        "Requirement:\n",
        "\n",
        "- A Gemini API key\n",
        "\n",
        "Gen AI SDK environment variables:\n",
        "\n",
        "- `GOOGLE_GENAI_USE_VERTEXAI=\"False\"`\n",
        "- `GOOGLE_API_KEY=\"<API_KEY>\"`\n",
        "\n",
        "Learn more about [getting a Gemini API key from Google AI Studio](https://aistudio.google.com/app/apikey).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZcmLBKqhdvX"
      },
      "source": [
        "ðŸ’¡ You can store your environment configuration outside of the source code:\n",
        "\n",
        "| Environment         | Method                                                      |\n",
        "| ------------------- | ----------------------------------------------------------- |\n",
        "| IDE                 | `.env` file (or equivalent)                                 |\n",
        "| Colab               | Colab Secrets (ðŸ—ï¸ icon in left panel, see code below)       |\n",
        "| Colab Enterprise    | Google Cloud project and location are automatically defined |\n",
        "| Vertex AI Workbench | Google Cloud project and location are automatically defined |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky2Escg3a1E2"
      },
      "source": [
        "Define the following environment detection functions. You can also define your configuration manually if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VTov81qlqoNz"
      },
      "outputs": [],
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from collections.abc import Callable\n",
        "\n",
        "from google import genai\n",
        "\n",
        "# Manual setup (leave unchanged if setup is environment-defined)\n",
        "\n",
        "# @markdown **Which API: Vertex AI or Google AI Studio?**\n",
        "GOOGLE_GENAI_USE_VERTEXAI = False  # @param {type: \"boolean\"}\n",
        "\n",
        "# @markdown **Option A - Google Cloud project [+location]**\n",
        "GOOGLE_CLOUD_PROJECT = \"\"  # @param {type: \"string\"}\n",
        "GOOGLE_CLOUD_LOCATION = \"global\"  # @param {type: \"string\"}\n",
        "\n",
        "# @markdown **Option B - Google AI Studio API key**\n",
        "GOOGLE_API_KEY = \"AIzaSyAhj78G6uVUnOsMdqsmnjE_h_f9AFjr2XA\"  # @param {type: \"string\"}\n",
        "\n",
        "\n",
        "def check_environment() -> bool:\n",
        "    check_colab_user_authentication()\n",
        "    return check_manual_setup() or check_vertex_ai() or check_colab() or check_local()\n",
        "\n",
        "\n",
        "def check_manual_setup() -> bool:\n",
        "    return check_define_env_vars(\n",
        "        GOOGLE_GENAI_USE_VERTEXAI,\n",
        "        GOOGLE_CLOUD_PROJECT.strip(),  # Might have been pasted with line return\n",
        "        GOOGLE_CLOUD_LOCATION,\n",
        "        GOOGLE_API_KEY,\n",
        "    )\n",
        "\n",
        "\n",
        "def check_vertex_ai() -> bool:\n",
        "    # Workbench and Colab Enterprise\n",
        "    match os.getenv(\"VERTEX_PRODUCT\", \"\"):\n",
        "        case \"WORKBENCH_INSTANCE\":\n",
        "            pass\n",
        "        case \"COLAB_ENTERPRISE\":\n",
        "            if not running_in_colab_env():\n",
        "                return False\n",
        "        case _:\n",
        "            return False\n",
        "\n",
        "    return check_define_env_vars(\n",
        "        True,\n",
        "        os.getenv(\"GOOGLE_CLOUD_PROJECT\", \"\"),\n",
        "        os.getenv(\"GOOGLE_CLOUD_REGION\", \"\"),\n",
        "        \"\",\n",
        "    )\n",
        "\n",
        "\n",
        "def check_colab() -> bool:\n",
        "    if not running_in_colab_env():\n",
        "        return False\n",
        "\n",
        "    # Colab Enterprise was checked before, so this is Colab only\n",
        "    from google.colab import auth as colab_auth  # type: ignore\n",
        "\n",
        "    colab_auth.authenticate_user()\n",
        "\n",
        "    # Use Colab Secrets (ðŸ—ï¸ icon in left panel) to store the environment variables\n",
        "    # Secrets are private, visible only to you and the notebooks that you select\n",
        "    # - Vertex AI: Store your settings as secrets\n",
        "    # - Google AI: Directly import your Gemini API key from the UI\n",
        "    vertexai, project, location, api_key = get_vars(get_colab_secret)\n",
        "\n",
        "    return check_define_env_vars(vertexai, project, location, api_key)\n",
        "\n",
        "\n",
        "def check_local() -> bool:\n",
        "    vertexai, project, location, api_key = get_vars(os.getenv)\n",
        "\n",
        "    return check_define_env_vars(vertexai, project, location, api_key)\n",
        "\n",
        "\n",
        "def running_in_colab_env() -> bool:\n",
        "    # Colab or Colab Enterprise\n",
        "    return \"google.colab\" in sys.modules\n",
        "\n",
        "\n",
        "def check_colab_user_authentication() -> None:\n",
        "    if running_in_colab_env():\n",
        "        from google.colab import auth as colab_auth  # type: ignore\n",
        "\n",
        "        colab_auth.authenticate_user()\n",
        "\n",
        "\n",
        "def get_colab_secret(secret_name: str, default: str) -> str:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "\n",
        "    try:\n",
        "        return userdata.get(secret_name)\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "\n",
        "def get_vars(getenv: Callable[[str, str], str]) -> tuple[bool, str, str, str]:\n",
        "    # Limit getenv calls to the minimum (may trigger UI confirmation for secret access)\n",
        "    vertexai_str = getenv(\"GOOGLE_GENAI_USE_VERTEXAI\", \"\")\n",
        "    if vertexai_str:\n",
        "        vertexai = vertexai_str.lower() in [\"true\", \"1\"]\n",
        "    else:\n",
        "        vertexai = bool(getenv(\"GOOGLE_CLOUD_PROJECT\", \"\"))\n",
        "\n",
        "    project = getenv(\"GOOGLE_CLOUD_PROJECT\", \"\") if vertexai else \"\"\n",
        "    location = getenv(\"GOOGLE_CLOUD_LOCATION\", \"\") if project else \"\"\n",
        "    api_key = getenv(\"GOOGLE_API_KEY\", \"\") if not project else \"\"\n",
        "\n",
        "    return vertexai, project, location, api_key\n",
        "\n",
        "\n",
        "def check_define_env_vars(\n",
        "    vertexai: bool,\n",
        "    project: str,\n",
        "    location: str,\n",
        "    api_key: str,\n",
        ") -> bool:\n",
        "    match (vertexai, bool(project), bool(location), bool(api_key)):\n",
        "        case (True, True, _, _):\n",
        "            # Vertex AI - Google Cloud project [+location]\n",
        "            location = location or \"global\"\n",
        "            define_env_vars(vertexai, project, location, \"\")\n",
        "        case (True, False, _, True):\n",
        "            # Vertex AI - API key\n",
        "            define_env_vars(vertexai, \"\", \"\", api_key)\n",
        "        case (False, _, _, True):\n",
        "            # Google AI Studio - API key\n",
        "            define_env_vars(vertexai, \"\", \"\", api_key)\n",
        "        case _:\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def define_env_vars(vertexai: bool, project: str, location: str, api_key: str) -> None:\n",
        "    os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = str(vertexai)\n",
        "    os.environ[\"GOOGLE_CLOUD_PROJECT\"] = project\n",
        "    os.environ[\"GOOGLE_CLOUD_LOCATION\"] = location\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "\n",
        "\n",
        "def check_configuration(client: genai.Client) -> None:\n",
        "    service = \"Vertex AI\" if client.vertexai else \"Google AI Studio\"\n",
        "    print(f\"Using the {service} API\", end=\"\")\n",
        "\n",
        "    if client._api_client.project:\n",
        "        print(f' with project \"{client._api_client.project[:7]}â€¦\"', end=\"\")\n",
        "        print(f' in location \"{client._api_client.location}\"')\n",
        "    elif client._api_client.api_key:\n",
        "        api_key = client._api_client.api_key\n",
        "        print(f' with API key \"{api_key[:5]}â€¦{api_key[-5:]}\"', end=\"\")\n",
        "        print(f\" (in case of error, make sure it was created for {service})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEYSAFqrqoNz"
      },
      "source": [
        "### ðŸ¤– Gen AI SDK\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q_Irs4D1EAf"
      },
      "source": [
        "To send Gemini requests, create a `google.genai` client:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DR77aUhzqoNz"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "\n",
        "check_environment()\n",
        "\n",
        "client = genai.Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcH5fQGBhdvY"
      },
      "source": [
        "Check your configuration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ANgm_y6_hdvY",
        "outputId": "a66cc11e-c7ca-4ac2-97d3-e2c2af082b24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the Google AI Studio API with API key \"AIzaSâ€¦jr2XA\" (in case of error, make sure it was created for Google AI Studio)\n"
          ]
        }
      ],
      "source": [
        "check_configuration(client)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vk9e4V_qoNz"
      },
      "source": [
        "### ðŸ§  Gemini model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acdi0aoIqoNz"
      },
      "source": [
        "Gemini comes in different [versions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models).\n",
        "\n",
        "Let's get started with Gemini 2.0 Flash, as it offers both high performance and low latency:\n",
        "\n",
        "- `GEMINI_2_0_FLASH = \"gemini-2.0-flash\"`\n",
        "\n",
        "> ðŸ’¡ We select Gemini 2.0 Flash intentionally. The Gemini 2.5 model family is generally available and even more capable, but we want to experiment and understand Gemini's core multimodal behavior. If we complete our challenge with 2.0, this should also work with newer models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9_PY-nla1E3"
      },
      "source": [
        "### âš™ï¸ Gemini configuration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AsCht_9a1E3"
      },
      "source": [
        "Gemini can be used in different ways, ranging from factual to creative mode. The problem we're trying to solve is a **data extraction** use case. We want results as factual and deterministic as possible. For this, we can change the [content generation parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/content-generation-parameters).\n",
        "\n",
        "We'll set the `temperature`, `top_p`, and `seed` parameters to minimize randomness:\n",
        "\n",
        "- `temperature=0.0`\n",
        "- `top_p=0.0`\n",
        "- `seed=42` (arbitrary fixed value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5dimkaC-CFe"
      },
      "source": [
        "### ðŸŽžï¸ Video sources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgrGTaPT-CFe"
      },
      "source": [
        "Here are the main video sources that Gemini can analyze:\n",
        "\n",
        "| source               | URI                                          | Vertex AI | Google AI Studio |\n",
        "| -------------------- | -------------------------------------------- | :-------: | :--------------: |\n",
        "| Google Cloud Storage | `gs://bucket/path/to/video.*`                |    âœ…     |                  |\n",
        "| Web URL              | `https://path/to/video.*`                    |    âœ…     |                  |\n",
        "| YouTube              | `https://www.youtube.com/watch?v=YOUTUBE_ID` |    âœ…     |        âœ…        |\n",
        "\n",
        "âš ï¸ Important notes\n",
        "\n",
        "- Our video test suite primarily uses public YouTube videos. This is for simplicity.\n",
        "- When analyzing YouTube sources, Gemini receives raw audio/video streams without any additional metadata, exactly as if processing the corresponding video files from Cloud Storage.\n",
        "- YouTube does offer caption/subtitle/transcript features (user-provided or auto-generated). However, these features focus on word-level speech-to-text and are limited to 40+ languages. Gemini does not receive any of this data and you'll see that a multimodal transcription with Gemini provides additional benefits.\n",
        "- Furthermore, our challenge also involves identifying speakers and extracting speaker data, a unique new capability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUBe2u8IqoNz"
      },
      "source": [
        "### ðŸ› ï¸ Helpers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsuIT4f1-CFe"
      },
      "source": [
        "Define our helper functions and data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Bus2ODLIK02F"
      },
      "outputs": [],
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "\n",
        "import enum\n",
        "from dataclasses import dataclass\n",
        "from datetime import timedelta\n",
        "\n",
        "import IPython.display\n",
        "import tenacity\n",
        "from google.genai.errors import ClientError\n",
        "from google.genai.types import (\n",
        "    FileData,\n",
        "    FinishReason,\n",
        "    GenerateContentConfig,\n",
        "    GenerateContentResponse,\n",
        "    Part,\n",
        "    VideoMetadata,\n",
        ")\n",
        "\n",
        "\n",
        "class Model(enum.Enum):\n",
        "    # Generally Available (GA)\n",
        "    GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\n",
        "    GEMINI_2_5_FLASH = \"gemini-2.5-flash\"\n",
        "    GEMINI_2_5_PRO = \"gemini-2.5-pro\"\n",
        "    # Default model\n",
        "    DEFAULT = GEMINI_2_0_FLASH\n",
        "\n",
        "\n",
        "# Default configuration for more deterministic outputs\n",
        "DEFAULT_CONFIG = GenerateContentConfig(\n",
        "    temperature=0.0,\n",
        "    top_p=0.0,\n",
        "    seed=42,  # Arbitrary fixed value\n",
        ")\n",
        "\n",
        "YOUTUBE_URL_PREFIX = \"https://www.youtube.com/watch?v=\"\n",
        "CLOUD_STORAGE_URI_PREFIX = \"gs://\"\n",
        "\n",
        "\n",
        "def url_for_youtube_id(youtube_id: str) -> str:\n",
        "    return f\"{YOUTUBE_URL_PREFIX}{youtube_id}\"\n",
        "\n",
        "\n",
        "class Video(enum.Enum):\n",
        "    pass\n",
        "\n",
        "\n",
        "class TestVideo(Video):\n",
        "    # For testing purposes, video duration is statically specified in the enum name\n",
        "    # Suffix (ISO 8601 based): _PT[<h>H][<m>M][<s>S]\n",
        "\n",
        "    # Google DeepMind | The Podcast | Season 3 Trailer | 59s\n",
        "    GDM_PODCAST_TRAILER_PT59S = url_for_youtube_id(\"0pJn3g8dfwk\")\n",
        "    # Google Maps | Walk in the footsteps of Jane Goodall | 2min 42s\n",
        "    JANE_GOODALL_PT2M42S = \"gs://cloud-samples-data/video/JaneGoodall.mp4\"\n",
        "    # Google DeepMind | AlphaFold | The making of a scientific breakthrough | 7min 54s\n",
        "    GDM_ALPHAFOLD_PT7M54S = url_for_youtube_id(\"gg7WjuFs8F4\")\n",
        "    # Brut | French reportage | 8min 28s\n",
        "    BRUT_FR_DOGS_WATER_LEAK_PT8M28S = url_for_youtube_id(\"U_yYkb-ureI\")\n",
        "    # Google DeepMind | The Podcast | AI for science | 54min 23s\n",
        "    GDM_AI_FOR_SCIENCE_FRONTIER_PT54M23S = url_for_youtube_id(\"nQKmVhLIGcs\")\n",
        "    # Google I/O 2025 | Developer Keynote | 1h 10min 03s\n",
        "    GOOGLE_IO_DEV_KEYNOTE_PT1H10M03S = url_for_youtube_id(\"GjvgtwSOCao\")\n",
        "    # Google Cloud | Next 2025 | Opening Keynote | 1h 40min 03s\n",
        "    GOOGLE_CLOUD_NEXT_PT1H40M03S = url_for_youtube_id(\"Md4Fs-Zc3tg\")\n",
        "    # Google I/O 2025 | Keynote | 1h 56min 35s\n",
        "    GOOGLE_IO_KEYNOTE_PT1H56M35S = url_for_youtube_id(\"o8NiE3XMPrM\")\n",
        "\n",
        "\n",
        "class ShowAs(enum.Enum):\n",
        "    DONT_SHOW = enum.auto()\n",
        "    TEXT = enum.auto()\n",
        "    MARKDOWN = enum.auto()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class VideoSegment:\n",
        "    start: timedelta\n",
        "    end: timedelta\n",
        "\n",
        "\n",
        "def generate_content(\n",
        "    prompt: str,\n",
        "    video: Video | None = None,\n",
        "    video_segment: VideoSegment | None = None,\n",
        "    model: Model | None = None,\n",
        "    config: GenerateContentConfig | None = None,\n",
        "    show_as: ShowAs = ShowAs.TEXT,\n",
        ") -> None:\n",
        "    prompt = prompt.strip()\n",
        "    model = model or Model.DEFAULT\n",
        "    config = config or DEFAULT_CONFIG\n",
        "\n",
        "    model_id = model.value\n",
        "    if video:\n",
        "        if not (video_part := get_video_part(video, video_segment)):\n",
        "            return\n",
        "        contents = [video_part, prompt]\n",
        "        caption = f\"{video.name} / {model_id}\"\n",
        "    else:\n",
        "        contents = prompt\n",
        "        caption = f\"{model_id}\"\n",
        "    print(f\" {caption} \".center(80, \"-\"))\n",
        "\n",
        "    for attempt in get_retrier():\n",
        "        with attempt:\n",
        "            response = client.models.generate_content(\n",
        "                model=model_id,\n",
        "                contents=contents,\n",
        "                config=config,\n",
        "            )\n",
        "            display_response_info(response)\n",
        "            display_response(response, show_as)\n",
        "\n",
        "\n",
        "def get_video_part(\n",
        "    video: Video,\n",
        "    video_segment: VideoSegment | None = None,\n",
        "    fps: float | None = None,\n",
        ") -> Part | None:\n",
        "    video_uri: str = video.value\n",
        "\n",
        "    if not client.vertexai:\n",
        "        video_uri = convert_to_https_url_if_cloud_storage_uri(video_uri)\n",
        "        if not video_uri.startswith(YOUTUBE_URL_PREFIX):\n",
        "            print(\"Google AI Studio API: Only YouTube URLs are currently supported\")\n",
        "            return None\n",
        "\n",
        "    file_data = FileData(file_uri=video_uri, mime_type=\"video/*\")\n",
        "    video_metadata = get_video_part_metadata(video_segment, fps)\n",
        "\n",
        "    return Part(file_data=file_data, video_metadata=video_metadata)\n",
        "\n",
        "\n",
        "def get_video_part_metadata(\n",
        "    video_segment: VideoSegment | None = None,\n",
        "    fps: float | None = None,\n",
        ") -> VideoMetadata:\n",
        "    def offset_as_str(offset: timedelta) -> str:\n",
        "        return f\"{offset.total_seconds()}s\"\n",
        "\n",
        "    if video_segment:\n",
        "        start_offset = offset_as_str(video_segment.start)\n",
        "        end_offset = offset_as_str(video_segment.end)\n",
        "    else:\n",
        "        start_offset = None\n",
        "        end_offset = None\n",
        "\n",
        "    return VideoMetadata(start_offset=start_offset, end_offset=end_offset, fps=fps)\n",
        "\n",
        "\n",
        "def convert_to_https_url_if_cloud_storage_uri(uri: str) -> str:\n",
        "    if uri.startswith(CLOUD_STORAGE_URI_PREFIX):\n",
        "        return f\"https://storage.googleapis.com/{uri.removeprefix(CLOUD_STORAGE_URI_PREFIX)}\"\n",
        "    return uri\n",
        "\n",
        "\n",
        "def get_retrier() -> tenacity.Retrying:\n",
        "    return tenacity.Retrying(\n",
        "        stop=tenacity.stop_after_attempt(7),\n",
        "        wait=tenacity.wait_incrementing(start=10, increment=1),\n",
        "        retry=should_retry_request,\n",
        "        reraise=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def should_retry_request(retry_state: tenacity.RetryCallState) -> bool:\n",
        "    if not retry_state.outcome:\n",
        "        return False\n",
        "    err = retry_state.outcome.exception()\n",
        "    if not isinstance(err, ClientError):\n",
        "        return False\n",
        "    print(f\"âŒ ClientError {err.code}: {err.message}\")\n",
        "\n",
        "    retry = False\n",
        "    match err.code:\n",
        "        case 400 if err.message is not None and \" try again \" in err.message:\n",
        "            # Workshop: project accessing Cloud Storage for the first time (service agent provisioning)\n",
        "            retry = True\n",
        "        case 429:\n",
        "            # Workshop: temporary project with 1 QPM quota\n",
        "            retry = True\n",
        "    print(f\"ðŸ”„ Retry: {retry}\")\n",
        "\n",
        "    return retry\n",
        "\n",
        "\n",
        "def display_response_info(response: GenerateContentResponse) -> None:\n",
        "    if usage_metadata := response.usage_metadata:\n",
        "        if usage_metadata.prompt_token_count:\n",
        "            print(f\"Input tokens   : {usage_metadata.prompt_token_count:9,d}\")\n",
        "        if usage_metadata.candidates_token_count:\n",
        "            print(f\"Output tokens  : {usage_metadata.candidates_token_count:9,d}\")\n",
        "        if usage_metadata.thoughts_token_count:\n",
        "            print(f\"Thoughts tokens: {usage_metadata.thoughts_token_count:9,d}\")\n",
        "    if not response.candidates:\n",
        "        print(\"âŒ No `response.candidates`\")\n",
        "        return\n",
        "    if (finish_reason := response.candidates[0].finish_reason) != FinishReason.STOP:\n",
        "        print(f\"âŒ {finish_reason = }\")\n",
        "    if not response.text:\n",
        "        print(\"âŒ No `response.text`\")\n",
        "        return\n",
        "\n",
        "\n",
        "def display_response(\n",
        "    response: GenerateContentResponse,\n",
        "    show_as: ShowAs,\n",
        ") -> None:\n",
        "    if show_as == ShowAs.DONT_SHOW:\n",
        "        return\n",
        "    if not (response_text := response.text):\n",
        "        return\n",
        "    response_text = response.text.strip()\n",
        "\n",
        "    print(\" start of response \".center(80, \"-\"))\n",
        "    match show_as:\n",
        "        case ShowAs.TEXT:\n",
        "            print(response_text)\n",
        "        case ShowAs.MARKDOWN:\n",
        "            display_markdown(response_text)\n",
        "    print(\" end of response \".center(80, \"-\"))\n",
        "\n",
        "\n",
        "def display_markdown(markdown: str) -> None:\n",
        "    IPython.display.display(IPython.display.Markdown(markdown))\n",
        "\n",
        "\n",
        "def display_video(video: Video) -> None:\n",
        "    video_url = convert_to_https_url_if_cloud_storage_uri(video.value)\n",
        "    assert video_url.startswith(\"https://\")\n",
        "\n",
        "    video_width = 600\n",
        "    if video_url.startswith(YOUTUBE_URL_PREFIX):\n",
        "        youtube_id = video_url.removeprefix(YOUTUBE_URL_PREFIX)\n",
        "        ipython_video = IPython.display.YouTubeVideo(youtube_id, width=video_width)\n",
        "    else:\n",
        "        ipython_video = IPython.display.Video(video_url, width=video_width)\n",
        "\n",
        "    display_markdown(f\"### Video ([source]({video_url}))\")\n",
        "    IPython.display.display(ipython_video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B499IGYqoN0"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ§ª Prototyping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbw8w44lqoN0"
      },
      "source": [
        "### ðŸŒ± Natural behavior\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGi_TzFzhj30"
      },
      "source": [
        "Before diving any deeper, it's interesting to see how Gemini responds to simple instructions, to develop some intuition about its natural behavior.\n",
        "\n",
        "Let's first see what we get with minimalistic prompts and a short English video.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gF3MCXLTqoN0",
        "outputId": "f84607c8-1ef6-4cb9-8dd9-55f10232a463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Video ([source](https://www.youtube.com/watch?v=0pJn3g8dfwk))"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x78ad1f2b3f50>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"600\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/0pJn3g8dfwk\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "image/jpeg": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------- GDM_PODCAST_TRAILER_PT59S / gemini-2.0-flash -----------------\n",
            "Input tokens   :    16,708\n",
            "Output tokens  :       421\n",
            "------------------------------ start of response -------------------------------\n",
            "[00:00:00] Do I have to call you Sir Demis now?\n",
            "[00:00:01] Oh, you don't.\n",
            "[00:00:03] Absolutely not.\n",
            "[00:00:04] Welcome to Google DeepMind the podcast with me, your host Professor Hannah Fry.\n",
            "[00:00:06] We want to take you to the heart of where these ideas are coming from.\n",
            "[00:00:12] We want to introduce you to the people who are leading the design of our collective future.\n",
            "[00:00:19] Getting the safety right is probably, I'd say, one of the most important challenges of our time.\n",
            "[00:00:25] I want safe and capable.\n",
            "[00:00:27] I want a bridge that will not collapse.\n",
            "[00:00:30] Just give these scientists a superpower that they had not imagined earlier.\n",
            "[00:00:34] autonomous vehicles.\n",
            "[00:00:35] It's hard to fathom that when you're working on a search engine.\n",
            "[00:00:38] We may see entirely new genre or entirely new forms of art come up.\n",
            "[00:00:42] There may be a new word that is not music, painting, photography, movie making, and that AI will have helped us create it.\n",
            "[00:00:48] You really want AGI to be able to peer into the mysteries of the universe.\n",
            "[00:00:51] Yes, quantum mechanics, string theory, well, and the nature of reality.\n",
            "[00:00:55] Ow.\n",
            "[00:00:57] The magic of AI.\n",
            "------------------------------- end of response --------------------------------\n"
          ]
        }
      ],
      "source": [
        "video = TestVideo.GDM_PODCAST_TRAILER_PT59S\n",
        "display_video(video)\n",
        "\n",
        "prompt = \"Transcribe the video's audio with time information.\"\n",
        "generate_content(prompt, video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jw70MYlqoN0"
      },
      "source": [
        "Results:\n",
        "\n",
        "- Gemini naturally outputs a list of `[time] transcript` lines.\n",
        "- That's Speech-to-Text in one line!\n",
        "- It looks like we can answer \"1ï¸âƒ£ What was said and when?\".\n",
        "\n",
        "Now, what about \"2ï¸âƒ£ Who are the speakers?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jOaSzf4TqoN0",
        "outputId": "0aa7708a-68c2-49ab-ff62-2dd25e26fb70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------- GDM_PODCAST_TRAILER_PT59S / gemini-2.0-flash -----------------\n",
            "Input tokens   :    16,705\n",
            "Output tokens  :        46\n",
            "------------------------------ start of response -------------------------------\n",
            "Here are the speakers identifiable in the video:\n",
            "\n",
            "*   Professor Hannah Fry\n",
            "*   Demis Hassabis\n",
            "*   Anca Dragan\n",
            "*   Pushmeet Kohli\n",
            "*   Jeff Dean\n",
            "*   Douglas Eck\n",
            "------------------------------- end of response --------------------------------\n"
          ]
        }
      ],
      "source": [
        "prompt = \"List the speakers identifiable in the video.\"\n",
        "generate_content(prompt, video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPX61D20qoN0"
      },
      "source": [
        "Results:\n",
        "\n",
        "- Gemini can consolidate the names visible on title cards during the video.\n",
        "- That's OCR + entity extraction in one line!\n",
        "- \"2ï¸âƒ£ Who are the speakers?\" looks solved too!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQtQ1vNiqoN0"
      },
      "source": [
        "### â© Not so fast!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkgHVqt2hj35"
      },
      "source": [
        "The natural next step is to jump to the final instructions, to solve our problem once and for all.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qF6BM_HfqoN1",
        "outputId": "4127d82b-8f94-438b-8c7d-da0d29c6b99f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------- GDM_PODCAST_TRAILER_PT59S / gemini-2.0-flash -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-3803613186.py\", line 8, in <cell line: 0>\n",
            "    generate_content(prompt, video)\n",
            "  File \"/tmp/ipython-input-2810389486.py\", line 105, in generate_content\n",
            "    for attempt in get_retrier():\n",
            "                   ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\", line 443, in __iter__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\", line 376, in iter\n",
            "    result = action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\", line 398, in <lambda>\n",
            "    self._add_action_func(lambda rs: rs.outcome.result())\n",
            "                                     ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/tmp/ipython-input-2810389486.py\", line 107, in generate_content\n",
            "    response = client.models.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/genai/models.py\", line 6535, in generate_content\n",
            "    response = self._generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/genai/models.py\", line 5347, in _generate_content\n",
            "    response = self._api_client.request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\", line 1289, in request\n",
            "    response = self._request(http_request, http_options, stream=False)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\", line 1109, in _request\n",
            "    return self._retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\", line 475, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\", line 376, in iter\n",
            "    result = action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\", line 398, in <lambda>\n",
            "    self._add_action_func(lambda rs: rs.outcome.result())\n",
            "                                     ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\", line 478, in __call__\n",
            "    result = fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\", line 1079, in _request_once\n",
            "    response = self._httpx_client.request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 825, in request\n",
            "    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 914, in send\n",
            "    response = self._send_handling_auth(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
            "    response = self._send_handling_redirects(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
            "    response = self._send_single_request(request)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_client.py\", line 1014, in _send_single_request\n",
            "    response = transport.handle_request(request)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\", line 250, in handle_request\n",
            "    resp = self._pool.handle_request(req)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n",
            "    raise exc from None\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n",
            "    response = connection.handle_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection.py\", line 103, in handle_request\n",
            "    return self._connection.handle_request(request)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\", line 136, in handle_request\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\", line 106, in handle_request\n",
            "    ) = self._receive_response_headers(**kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\", line 177, in _receive_response_headers\n",
            "    event = self._receive_event(timeout=timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\", line 217, in _receive_event\n",
            "    data = self._network_stream.read(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/httpcore/_backends/sync.py\", line 128, in read\n",
            "    return self._sock.recv(max_bytes)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/ssl.py\", line 1232, in recv\n",
            "    return self.read(buflen)\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/ssl.py\", line 1105, in read\n",
            "    return self._sslobj.read(len)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1771, in getinnerframes\n",
            "    framelist.append(FrameInfo(*frameinfo, positions=traceback_info.positions))\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3803613186.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2810389486.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(prompt, video, video_segment, model, config, show_as)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_retrier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_explicit_retry\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_run_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2810389486.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(prompt, video, video_segment, model, config, show_as)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             response = client.models.generate_content(\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   6534\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6535\u001b[0;31m       response = self._generate_content(\n\u001b[0m\u001b[1;32m   6536\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparsed_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5347\u001b[0;31m     response = self._api_client.request(\n\u001b[0m\u001b[1;32m   5348\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     )\n\u001b[0;32m-> 1289\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m     response_body = (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_explicit_retry\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_run_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request_once\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m   1078\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m       response = self._httpx_client.request(\n\u001b[0m\u001b[1;32m   1080\u001b[0m           \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    824\u001b[0m         )\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1231\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Transcribe the video's audio including speaker names (use \"?\" if not found).\n",
        "If not found, but you are sure they are different people use different symbols to show differentation\n",
        "\n",
        "Format example:\n",
        "[00:02] John Doe - Hello Alice!\n",
        "\"\"\"\n",
        "generate_content(prompt, video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I68y1ZBUqoN1"
      },
      "source": [
        "This is almost correct. The first segment isn't attributed to the host (who is only introduced a bit later), but everything else looks correct.\n",
        "\n",
        "Nonetheless, these are not real-world conditions:\n",
        "\n",
        "- The video is very short (less than a minute)\n",
        "- The video is also rather simple (speakers are clearly introduced with on-screen title cards)\n",
        "\n",
        "Let's try with this 8-minute (and more complex) video:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ycy--DT4qoN1",
        "outputId": "5843d692-392c-4018-a5e8-63f6873e35ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------- GDM_ALPHAFOLD_PT7M54S / gemini-2.0-flash -------------------\n",
            "Input tokens   :   134,177\n",
            "Output tokens  :     2,689\n",
            "------------------------------ start of response -------------------------------\n",
            "[00:02] ? - We've discovered more about the world than any other civilization before us.\n",
            "[00:08] ? - But we have been stuck on this one problem.\n",
            "[00:11] ? - How do proteins fold up?\n",
            "[00:13] ? - How do proteins go from a string of amino acids to a compact shape that acts as a machine and drives life?\n",
            "[00:22] ? - When you find out about proteins, it is very exciting.\n",
            "[00:25] ? - You could think of them as little biological nano machines.\n",
            "[00:28] ? - They are essentially the fundamental building blocks that power everything living on this planet.\n",
            "[00:34] ? - If we can reliably predict protein structures using AI, that could change the way we understand the natural world.\n",
            "[00:46] ? - Protein folding is one of these holy grail type problems in biology.\n",
            "[00:50] Demis Hassabis - We've always hypothesized that AI should be helpful to make these kinds of big scientific breakthroughs more quickly.\n",
            "[00:58] ? - And then I'll probably be looking at little tunings that might make a difference.\n",
            "[01:02] ? - It should be creating a histogram on and a background skill.\n",
            "[01:04] ? - We've been working on our system AlphaFold really hard now for over two years.\n",
            "[01:08] ? - Rather than having to do painstaking experiments, in the future biologists might be able to instead rely on AI methods to directly predict structures quickly and efficiently.\n",
            "[01:17] Kathryn Tunyasuvunakool - Generally speaking, biologists tend to be quite skeptical of computational work, and I think that skepticism is healthy and I respect it, but I feel very excited about what AlphaFold can achieve.\n",
            "[01:28] Andrew Senior - CASP is when we, we say, look, DeepMind is doing protein folding.\n",
            "[01:31] Andrew Senior - This is how good we are, and maybe it's better than everybody else, maybe it isn't.\n",
            "[01:37] ? - We decided to enter CASP competition because it represented the Olympics of protein folding.\n",
            "[01:44] John Moult - CASP, we started to try and speed up the solution to the protein folding problem.\n",
            "[01:50] John Moult - When we started CASP in 1994, I certainly was naive about how hard this was going to be.\n",
            "[01:58] ? - It was very cumbersome to do that because it took a long time.\n",
            "[02:01] ? - Let's see what, what, what are we doing still to improve?\n",
            "[02:03] ? - Typically 100 different groups from around the world participate in CASP, and we take a set of 100 proteins and we ask the groups to send us what they think the structures look like.\n",
            "[02:15] ? - We can reach 57.9 GDT on CASP 12 ground truth.\n",
            "[02:19] John Jumper - CASP has a metric on which you will be scored, which is this GDT metric.\n",
            "[02:25] John Jumper - On a scale of zero to 100, you would expect a GDT over 90 to be a solution to the problem.\n",
            "[02:33] ? - If we do achieve this, this has incredible medical relevance.\n",
            "[02:37] Pushmeet Kohli - The implications are immense, from how diseases progress, how you can discover new drugs.\n",
            "[02:45] Pushmeet Kohli - It's endless.\n",
            "[02:46] ? - I wanted to make a, a really simple system and the results have been surprisingly good.\n",
            "[02:50] ? - The team got some results with a new technique, not only is it more accurate, but it's much faster than the old system.\n",
            "[02:56] ? - I think we'll substantially exceed what we're doing right now.\n",
            "[02:59] ? - This is a game, game changer, I think.\n",
            "[03:01] John Moult - In CASP 13, something very significant had happened.\n",
            "[03:06] John Moult - For the first time, we saw the effective application of artificial intelligence.\n",
            "[03:11] ? - We've advanced the state of the art in the field, so that's fantastic, but we still got a long way to go before we've solved it.\n",
            "[03:18] ? - The shapes were now approximately correct for many of the proteins, but the details, exactly where each atom sits, which is really what we would call a solution, we're not yet there.\n",
            "[03:29] ? - It doesn't help if you have the tallest ladder when you're going to the moon.\n",
            "[03:33] ? - We hit a little bit of a brick wall, um, since we won CASP, then it was back to the drawing board and like what are our new ideas?\n",
            "[03:41] ? - Um, and then it's taken a little while, I would say, for them to get back to where they were, but with the new ideas.\n",
            "[03:51] ? - They can go further, right?\n",
            "[03:52] ? - So, um, so that's a really important moment.\n",
            "[03:55] ? - I've seen that moment so many times now, but I know what that means now, and I know this is the time now to press.\n",
            "[04:02] ? - We need to double down and go as fast as possible from here.\n",
            "[04:05] ? - I think we've got no time to lose.\n",
            "[04:07] ? - So the intention is to enter CASP again.\n",
            "[04:09] ? - CASP is deeply stressful.\n",
            "[04:12] ? - There's something weird going on with, um, the learning because it is learning something that's correlated with GDT, but it's not calibrated.\n",
            "[04:18] ? - I feel slightly uncomfortable.\n",
            "[04:20] ? - We should be learning this, you know, in the blink of an eye.\n",
            "[04:23] ? - The technology advancing outside DeepMind is also doing incredible work.\n",
            "[04:27] Richard Evans - And there's always the possibility another team has come somewhere out there field that we don't even know about.\n",
            "[04:32] ? - Someone asked me, well, should we panic now?\n",
            "[04:33] ? - Of course, we should have been panicking before.\n",
            "[04:35] ? - It does seem to do better, but still doesn't do quite as well as the best model.\n",
            "[04:39] ? - Um, so it looks like there's room for improvement.\n",
            "[04:42] ? - There's always a risk that you've missed something, and that's why blind assessments like CASP are so important to validate whether our results are real.\n",
            "[04:49] ? - Obviously, I'm excited to see how CASP 14 goes.\n",
            "[04:51] ? - My expectation is we get our heads down, we focus on the full goal, which is to solve the whole problem.\n",
            "[05:14] ? - We were prepared for CASP to start on April 15th because that's when it was originally scheduled to start, and it's been delayed by a month due to coronavirus.\n",
            "[05:24] ? - I really miss everyone.\n",
            "[05:25] ? - No, I struggled a little bit just kind of getting into a routine, especially, uh, my wife, she came down with the, the virus.\n",
            "[05:32] ? - I mean, luckily it didn't turn out too serious.\n",
            "[05:34] ? - CASP started on Monday.\n",
            "[05:37] Demis Hassabis - Can I just check this diagram you've got here, John, this one where we ask ground truth.\n",
            "[05:40] Demis Hassabis - Is this one we've done badly on?\n",
            "[05:42] ? - We're actually quite good on this region.\n",
            "[05:43] ? - If you imagine that we hadn't have said it came around this way, but had put it in.\n",
            "[05:47] ? - Yeah, and that instead.\n",
            "[05:48] ? - Yeah.\n",
            "[05:49] ? - One of the hardest proteins we've gotten in CASP thus far is a SARS-CoV-2 protein, uh, called Orf8.\n",
            "[05:55] ? - Orf8 is a coronavirus protein.\n",
            "[05:57] ? - We tried really hard to improve our prediction, like really, really hard, probably the most time that we have ever spent on a single target.\n",
            "[06:05] ? - So we're about two-thirds of the way through CASP, and we've gotten three answers back.\n",
            "[06:11] ? - We now have a ground truth for Orf8, which is one of the coronavirus proteins.\n",
            "[06:17] ? - And it turns out we did really well in predicting that.\n",
            "[06:20] Demis Hassabis - Amazing job, everyone, the whole team.\n",
            "[06:23] Demis Hassabis - It's been an incredible effort.\n",
            "[06:24] John Moult - Here what we saw in CASP 14 was a group delivering atomic accuracy off the bat, essentially solving what in our world is two problems.\n",
            "[06:34] John Moult - How do you look to find the right solution, and then how do you recognize you've got the right solution when you're there?\n",
            "[06:41] ? - All right, are we, are we mostly here?\n",
            "[06:46] ? - I'm going to read an email.\n",
            "[06:48] ? - Uh, I got this from John Moult.\n",
            "[06:50] ? - Now I'll just read it.\n",
            "[06:51] ? - It says, John, as I expect you know, your group has performed amazingly well in CASP 14, both relative to other groups and in absolute model accuracy.\n",
            "[07:02] ? - Congratulations on this work.\n",
            "[07:05] ? - It is really outstanding.\n",
            "[07:07] Demis Hassabis - AlphaFold represents a huge leap forward that I hope will really accelerate drug discovery and help us to better understand disease.\n",
            "[07:13] John Moult - It's pretty mind-blowing.\n",
            "[07:16] John Moult - You know, these results were, for me, having worked on this problem so long, after many, many stops and starts and will this ever get there, suddenly this is a solution.\n",
            "[07:28] John Moult - We've solved the problem.\n",
            "[07:29] John Moult - This gives you such excitement about the way science works, about how you can never see exactly or even approximately what's going to happen next.\n",
            "[07:37] John Moult - There are always these surprises, and that really, as a scientist, is what keeps you going.\n",
            "[07:41] John Moult - What's going to be the next surprise?\n",
            "------------------------------- end of response --------------------------------\n"
          ]
        }
      ],
      "source": [
        "generate_content(prompt, TestVideo.GDM_ALPHAFOLD_PT7M54S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh0XSXOtqoN1"
      },
      "source": [
        "This falls apart: Most segments have no identified speaker!\n",
        "\n",
        "As we are trying to solve a new complex problem, LLMs haven't been trained on any known solution. This is likely why direct instructions do not yield the expected answer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La1_j-f7f0uH"
      },
      "source": [
        "### ðŸš§ Experiment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJEQDPGGhj35"
      },
      "source": [
        "Let's take a few minutes to experiment:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hRPVMKZCf0uH",
        "outputId": "ab323573-fc1b-42b1-a6e4-2184a18775eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------- GDM_ALPHAFOLD_PT7M54S / gemini-2.0-flash -------------------\n",
            "Input tokens   :   134,212\n",
            "Output tokens  :     2,636\n",
            "------------------------------ start of response -------------------------------\n",
            "Here is the transcription of the video:\n",
            "\n",
            "[00:02] ? - Weâ€™ve discovered more about the world than any other civilization before us.\n",
            "[00:07] ? - But we have been stuck on this one problem.\n",
            "[00:11] ? - How do proteins fold up?\n",
            "[00:14] ? - How do proteins go from a string of amino acids to a compact shape that acts as a machine and drives life?\n",
            "[00:22] ? - When you find out about proteins, it is very exciting.\n",
            "[00:25] ? - You could think of them as little biological nano machines.\n",
            "[00:28] ? - They are essentially the fundamental building blocks that power everything living on this planet.\n",
            "[00:34] ? - If we can reliably predict protein structures using AI, that could change the way we understand the natural world.\n",
            "[00:46] ? - Protein folding is one of these holy grail type problems in biology.\n",
            "[00:50] Demis Hassabis (wearing a gray sweater and glasses) - Weâ€™ve always hypothesized that AI should be helpful to make these kinds of big scientific breakthroughs more quickly.\n",
            "[00:58] ? - And then Iâ€™ll probably be looking at little tunings that might make a difference.\n",
            "[01:02] ? - It should be creating a histogram on and a background skill.\n",
            "[01:04] ? - Weâ€™ve been working on our system AlphaFold really hard now for over two years.\n",
            "[01:08] ? - Rather than having to do painstaking experiments, in the future biologists might be able to instead rely on AI methods to directly predict structures quickly and efficiently.\n",
            "[01:17] Kathryn Tunyasuvunakool (wearing a striped shirt and glasses) - Generally speaking, biologists tend to be quite skeptical of computational work, and I think that skepticism is healthy and I respect it, but I feel very excited about what AlphaFold can achieve.\n",
            "[01:28] Andrew Senior (wearing a gray shirt and glasses) - CASP is when we, we say, look, DeepMind is doing protein folding, this is how good we are, and maybe itâ€™s better than everybody else, maybe it isnâ€™t.\n",
            "[01:37] ? - We decided to enter CASP competition because it represented the Olympics of protein folding.\n",
            "[01:44] John Moult (wearing a blue shirt and glasses) - CASP, we started to try and speed up the solution to the protein folding problem.\n",
            "[01:50] John Moult - When we started CASP in 1994, I certainly was naive about how hard this was going to be.\n",
            "[01:58] ? - It was very cumbersome to do that because it took a long time.\n",
            "[02:01] ? - Letâ€™s see what, what, what weâ€™re doing still to improve.\n",
            "[02:03] ? - Typically 100 different groups from around the world participate in CASP, and we take a set of 100 proteins and we ask the groups to send us what they think the structures look like.\n",
            "[02:15] ? - We can reach 57.9 GDT on CASP 12 ground truth.\n",
            "[02:20] John Jumper (wearing a red shirt) - CASP has a metric on which you will be scored, which is this GDT metric.\n",
            "[02:25] ? - On a scale of zero to 100, you would expect a GDT over 90 to be a solution to the problem.\n",
            "[02:34] ? - If we do achieve this, this has incredible medical relevance.\n",
            "[02:37] Pushmeet Kohli (wearing a red turban and a red jacket) - The implications are immense, from how diseases progress, how you can discover new drugs.\n",
            "[02:45] ? - Itâ€™s endless.\n",
            "[02:46] ? - I wanted to make a really simple system and the results have been surprisingly good.\n",
            "[02:50] ? - The team got some results with a new technique, not only is it more accurate, but itâ€™s much faster than the old system.\n",
            "[02:56] ? - I think weâ€™ll substantially exceed what weâ€™re doing right now.\n",
            "[02:59] ? - This is a game, game changer, I think.\n",
            "[03:02] John Moult - In CASP 13, something very significant had happened.\n",
            "[03:08] John Moult - For the first time, we saw the effective application of artificial intelligence.\n",
            "[03:11] ? - Weâ€™ve advanced the state of the art in the field, so thatâ€™s fantastic, but we still got a long way to go before weâ€™ve solved it.\n",
            "[03:17] ? - The shapes were now approximately correct for many of the proteins, but the details, exactly where each atom sits, which is really what we would call a solution, were not yet there.\n",
            "[03:30] ? - It doesnâ€™t help if you have the tallest ladder when youâ€™re going to the moon.\n",
            "[03:34] ? - We hit a little bit of a brick wall, um, since we won CASP, then it was back to the drawing board and like what are our new ideas, um, and then itâ€™s taken a little while, I would say, for them to get back to where they were, but with the new ideas, and then now I think weâ€™re seeing the benefits of the new ideas, they can go further, right?\n",
            "[04:01] ? - So, um, so thatâ€™s a really important moment.\n",
            "[04:01] ? - Iâ€™ve seen that moment so many times now, but I know what that means now, and I know this is the time now to press.\n",
            "[04:02] ? - We need to double down and go as fast as possible from here.\n",
            "[04:05] ? - I think weâ€™ve got no time to lose.\n",
            "[04:07] ? - So the intention is to enter CASP again.\n",
            "[04:09] ? - CASP is deeply stressful.\n",
            "[04:12] ? - Thereâ€™s something weird going on with, um, the learning, because it is learning something thatâ€™s correlated with GDT, but itâ€™s not calibrated.\n",
            "[04:18] ? - I feel slightly uncomfortable.\n",
            "[04:20] ? - We should be learning this, you know, in the blink of an eye.\n",
            "[04:23] ? - The technology advancing outside DeepMind is also doing incredible work.\n",
            "[04:27] Richard Evans (wearing a black shirt and glasses) - Thereâ€™s always the possibility another team has come somewhere out of left field that we donâ€™t even know about.\n",
            "[04:32] ? - Someone asked me, well, should we panic now?\n",
            "[04:33] ? - Of course, we should have been panicking before.\n",
            "[04:35] ? - It does seem to do better, but still doesnâ€™t do quite as well as the best model.\n",
            "[04:41] ? - Um, so it looks like thereâ€™s room for improvement.\n",
            "[04:43] ? - Thereâ€™s always a risk that youâ€™ve missed something, and thatâ€™s why blind assessments like CASP are so important to validate whether our results are real.\n",
            "[04:49] ? - Obviously Iâ€™m excited to see how CASP 14 goes.\n",
            "[04:51] ? - My expectation is we get our heads down, we focus on the full goal, which is to solve the whole problem.\n",
            "[05:01] ? - I must give the British people a very simple instruction.\n",
            "[05:12] ? - You must stay at home.\n",
            "[05:14] ? - We were prepared for CASP to start on April 15th, because thatâ€™s when it was originally scheduled to start, and itâ€™s been delayed by a month due to coronavirus.\n",
            "[05:24] ? - I really miss everyone.\n",
            "[05:25] ? - No, I struggled a little bit, just kind of getting into a routine, especially, uh, my wife, she came down with the, the virus.\n",
            "[05:32] ? - I mean, luckily it didnâ€™t turn out too serious.\n",
            "[05:34] ? - CASP started on Monday.\n",
            "[05:37] Demis Hassabis - Can I just check this diagram youâ€™ve got here, John, this one where we ask ground truth, is this one weâ€™ve done badly on?\n",
            "[05:43] John Jumper - Weâ€™re actually quite good on this region.\n",
            "[05:45] John Jumper - If you imagine that we hadnâ€™t have said it came around this way, but had put it in there instead.\n",
            "[05:47] Demis Hassabis - Yeah, and that instead, yeah.\n",
            "[05:49] John Jumper - One of the hardest proteins weâ€™ve gotten in CASP thus far is a SARS-CoV-2 protein, uh, called Orf8.\n",
            "[05:55] Kathryn Tunyasuvunakool - Orf8 is a coronavirus protein.\n",
            "[05:57] Kathryn Tunyasuvunakool - We tried really hard to improve our prediction, like really, really hard, probably the most time that we have ever spent on a single target.\n",
            "[06:05] John Jumper - So weâ€™re about two thirds of the way through CASP and weâ€™ve gotten three answers back.\n",
            "[06:10] Kathryn Tunyasuvunakool - We now have a ground truth for Orf8, which is one of the coronavirus proteins.\n",
            "[06:17] Kathryn Tunyasuvunakool - And it turns out we did really well in predicting that.\n",
            "[06:20] Demis Hassabis - Amazing job, everyone, the whole team.\n",
            "[06:24] Demis Hassabis - Itâ€™s been an incredible effort.\n",
            "[06:24] John Moult - Here what we saw in CASP 14 was a group delivering atomic accuracy off the bat, essentially solving what in our world is two problems.\n",
            "[06:34] John Moult - How do you look to find the right solution and then how do you recognize youâ€™ve got the right solution when youâ€™re there?\n",
            "[06:41] John Moult - All right, are we, are we mostly here?\n",
            "[07:06] Demis Hassabis - AlphaFold represents a huge leap forward that I hope will really accelerate drug discovery and help us to better understand disease.\n",
            "[07:15] John Moult - Itâ€™s pretty mind blowing.\n",
            "[07:16] John Moult - You know, these results were, for me, having worked on this problem so long, after many, many stops and starts and will this ever get there, suddenly this is a solution, weâ€™ve solved the problem.\n",
            "[07:28] John Moult - This gives you such excitement about the way science works, about how you can never see exactly or even approximately whatâ€™s going to happen next.\n",
            "[07:37] John Moult - There are always these surprises and that really, as a scientist, is what keeps you going.\n",
            "[07:42] John Moult - Whatâ€™s the going to be the next surprise?\n",
            "------------------------------- end of response --------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Experiment with your own instructions to improve the response\n",
        "prompt = \"\"\"\n",
        "Transcribe the video, including speaker names (use \"?\" if not found).\n",
        "\n",
        "If not found, but you are sure they are different people use different symbols to show differentation\n",
        "\n",
        "Format example:\n",
        "[00:02] John Doe - Hello Alice!\n",
        "\n",
        "Also mention in parantheses, what the outfit of the speakers look like.\n",
        "\"\"\"\n",
        "# short and easier video\n",
        "#generate_content(prompt, TestVideo.GDM_PODCAST_TRAILER_PT59S)\n",
        "\n",
        "# If it works on the short video, also check your prompt on this more complex video\n",
        "generate_content(prompt, TestVideo.GDM_ALPHAFOLD_PT7M54S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bUYsYBff0uH"
      },
      "source": [
        "Did you find a solution? Does it also work with the more complex video? Did you double-check by watching the whole video? If so, congratulations, you can proceed to \"Structured output\".\n",
        "\n",
        "Otherwise, at this stage:\n",
        "\n",
        "- We might conclude that we can't solve the problem with real-world videos.\n",
        "- Persevering by trying more and more elaborate prompts for this unsolved problem might result in a waste of time.\n",
        "\n",
        "Let's take a step back and think about what happens under the hoodâ€¦\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "784a-K1wqoN1"
      },
      "source": [
        "---\n",
        "\n",
        "## âš›ï¸ Under the hood\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmvfnIAwa1E4"
      },
      "source": [
        "Modern LLMs are mostly built upon the Transformer architecture, a new neural network design detailed in a 2017 paper by Google researchers titled [Attention Is All You Need](https://arxiv.org/abs/1706.03762). The paper introduced the self-attention mechanism, a key innovation that fundamentally changed the way machines process language.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APobp3nihj35"
      },
      "source": [
        "### ðŸª™ Tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2gSrBCPqoN1"
      },
      "source": [
        "Tokens are the LLM building blocks. We can consider a token to represent a piece of information.\n",
        "\n",
        "Examples of Gemini multimodal tokens (with default parameters):\n",
        "\n",
        "| content                 |   tokens    | details                                               |\n",
        "| ----------------------- | :---------: | ----------------------------------------------------- |\n",
        "| `hello`                 |      1      | 1 token for common words/sequences                    |\n",
        "| `passionately`          |      2      | `passionâ€¢ately`                                       |\n",
        "| `passionnÃ©ment`         |      3      | `passionâ€¢nÃ©â€¢ment` (same adverb in French)             |\n",
        "| image                   |     258     | per image (or per tile depending on image resolution) |\n",
        "| audio without timecodes | 25 / second | handled by the audio tokenizer                        |\n",
        "| video without audio     | 258 / frame | handled by the video tokenizer at 1 frame per second  |\n",
        "| `MM:SS` timecode        |      5      | audio chunk or video frame temporal reference         |\n",
        "| `H:MM:SS` timecode      |      7      | similarly, for content longer than 1 hour             |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE8QUvL2hj35"
      },
      "source": [
        "### ðŸŽžï¸ Sampling frame rate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgYLJiWGhj35"
      },
      "source": [
        "By default, video frames are sampled at 1 frame per second (1 FPS). These frames are included in the context with their corresponding timecodes.\n",
        "\n",
        "You can use a custom sampling frame rate with the `Part.video_metadata.fps` parameter:\n",
        "\n",
        "| video type    | change                  | `fps` range         |\n",
        "| ------------- | ----------------------- | ------------------- |\n",
        "| static, slow  | decrease the frame rate | `0.0 < fps < 1.0`   |\n",
        "| dynamic, fast | increase the frame rate | `1.0 < fps <= 24.0` |\n",
        "\n",
        "> ðŸ’¡ For `1.0 < fps`, Gemini was trained to understand `MM:SS.sss` and `H:MM:SS.sss` timecodes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHomr1bGhj35"
      },
      "source": [
        "### ðŸ” Media resolution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2hjOIOkhj35"
      },
      "source": [
        "By default, each sampled frame is represented with 258 tokens.\n",
        "\n",
        "You can specify a medium or low media resolution with the `GenerateContentConfig.media_resolution` parameter:\n",
        "\n",
        "| `media_resolution` for video inputs | tokens/frame | benefit                                              |\n",
        "| ----------------------------------- | -----------: | ---------------------------------------------------- |\n",
        "| `MEDIA_RESOLUTION_MEDIUM` (default) |          258 | higher precision, allows more detailed understanding |\n",
        "| `MEDIA_RESOLUTION_LOW`              |           66 | faster and cheaper inference, allows longer videos   |\n",
        "\n",
        "> ðŸ’¡ The \"media resolution\" can be seen as the \"image token resolution\": the number of tokens used to represent an image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpWr_dwDhj36"
      },
      "source": [
        "### ðŸ§® Probabilities all the way down\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzkrIwznqoN1"
      },
      "source": [
        "The ability of LLMs to communicate in flawless natural language is very impressive, but it's easy to get carried away and make incorrect assumptions.\n",
        "\n",
        "Keep in mind how LLMs work:\n",
        "\n",
        "- An LLM is trained on a massive tokenized dataset, which represents its knowledge (its long-term memory)\n",
        "- During the training, its neural network learns token patterns\n",
        "- When you send a request to an LLM, your inputs are transformed into tokens (tokenization)\n",
        "- To answer your request, the LLM predicts, token by token, the next likely tokens\n",
        "- Overall, LLMs are exceptional statistical token prediction machines that seem to mimic how some parts of our brain work\n",
        "\n",
        "This has a few consequences:\n",
        "\n",
        "- LLM outputs are just statistically likely follow-ups to your inputs\n",
        "- LLMs show some forms of reasoning: they can match complex patterns but have no actual deep understanding\n",
        "- LLMs have no consciousness: they are designed to generate tokens and will do so based on your instructions\n",
        "- Order matters: Tokens that are generated first will influence tokens that are generated next\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-bN1DcQqoN2"
      },
      "source": [
        "For the next step, some methodical prompt crafting might helpâ€¦\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w2owFhrqoN2"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ—ï¸ Prompt crafting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3lIFIj5qoN2"
      },
      "source": [
        "### ðŸªœ Methodology\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMklaeV3qoN2"
      },
      "source": [
        "Prompt crafting, also called prompt engineering, is a relatively new field. It involves designing and refining text instructions to guide LLMs towards generating desired outputs. Like writing, it is both an art and a science, a skill that everyone can develop with practice.\n",
        "\n",
        "We can find countless reference materials about prompt crafting. Some prompts can be very long, complex, and even scary. Crafting prompts with a high-performing LLM like Gemini is much simpler. Here are three key adjectives to keep in mind:\n",
        "\n",
        "- iterative\n",
        "- precise\n",
        "- concise\n",
        "\n",
        "**Iterative**\n",
        "\n",
        "Prompt crafting is typically an iterative process. Here are some recommendations:\n",
        "\n",
        "- Craft your prompt step by step\n",
        "- Keep track of your successive iterations\n",
        "- At every iteration, make sure to measure what's working versus what's not\n",
        "- If you reach a regression, backtrack to a successful iteration\n",
        "\n",
        "**Precise**\n",
        "\n",
        "Precision is key:\n",
        "\n",
        "- Use words as specific as possible\n",
        "- Words with multiple meanings can introduce variability, so use precise expressions\n",
        "- Precision will influence probabilities in your favor\n",
        "\n",
        "**Concise**\n",
        "\n",
        "Concision has additional advantages:\n",
        "\n",
        "- A short prompt is easier for us developers to understand (and maintain!)\n",
        "- The longer your prompt is, the more likely you are to introduce inconsistencies or even contradictions, which results in variable interpretations of your instructions\n",
        "- Test and trust the LLM's knowledge: this knowledge acts as an implicit context and can make your prompt shorter and clearer\n",
        "\n",
        "Overall, though this may seem contradictory, if you take the time to be iterative, precise, and concise, you are likely to save a lot of time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq7RLbwohj36"
      },
      "source": [
        "> ðŸ’¡ If you want to explore this topic, check out [Prompting strategies](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-design-strategies) (Google Cloud reference) and [Prompt engineering](https://www.kaggle.com/whitepaper-prompt-engineering) (68-page PDF by Lee Boonstra).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWcKgdh9hj36"
      },
      "source": [
        "### ðŸ“š Terminology\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQOEU3wLqoN2"
      },
      "source": [
        "We're not experts in video transcription (yet!) but we want Gemini to behave as one. Consequently, we'd like to write prompts as specific as possible for this use case. While LLMs process instructions based on their training knowledge, they can also share this knowledge with us.\n",
        "\n",
        "We can learn a lot by directly asking Gemini:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "K8Q6dKvRqoN2",
        "outputId": "2bcd5f99-4f90-4d05-dd96-45f5975eac75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------- gemini-2.0-flash -------------------------------\n",
            "Input tokens   :        18\n",
            "Output tokens  :     1,110\n",
            "------------------------------ start of response -------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Terminology Used for Video Transcriptions\n\nHere's a breakdown of common terminology used when discussing video transcriptions:\n\n*   **Transcription:** The general process of converting audio or video content into text.\n*   **Video Transcription:** Specifically refers to transcribing the audio portion of a video.\n*   **Verbatim Transcription:** A transcription that includes every spoken word, including filler words (\"um,\" \"ah\"), false starts, stutters, and background noises (if relevant).  It aims for a highly accurate and complete record.\n*   **Clean Verbatim Transcription (or Edited Transcription):** A transcription that removes filler words, false starts, and stutters to create a more readable and polished text.  It focuses on conveying the core message clearly.\n*   **Intelligent Verbatim Transcription:** Similar to clean verbatim, but may also correct minor grammatical errors or rephrase sentences slightly for clarity, while still preserving the speaker's intent and style.\n*   **Timecoding (or Timestamping):** Adding timestamps to the transcription to indicate when specific words or phrases were spoken in the video.  This is crucial for navigation and referencing specific moments.\n*   **Speaker Identification:** Identifying and labeling different speakers in the transcription (e.g., Speaker 1, Speaker 2, or using names if known).\n*   **Captioning:** Creating text versions of the audio content that are synchronized with the video for accessibility.  Captions are typically displayed on the screen.\n*   **Subtitles:** Similar to captions, but often used for translating the audio into a different language.\n*   **SRT (SubRip Subtitle) file:** A common file format for subtitles and captions, containing the text and timing information.\n*   **VTT (Video Text Tracks) file:** A more advanced file format for subtitles and captions, offering more styling options and metadata.\n*   **Burned-in Captions (or Open Captions):** Captions that are permanently embedded in the video and cannot be turned off.\n*   **Closed Captions:** Captions that can be turned on or off by the viewer.\n*   **Automatic Speech Recognition (ASR):** The technology used to automatically transcribe audio or video.  ASR systems are constantly improving but often require human review and editing for accuracy.\n*   **Human Transcription:** Transcription performed by a human transcriber, which generally results in higher accuracy than ASR alone.\n\n## Typical Output Example\n\nHere's an example of a **clean verbatim transcription** with **timecoding** and **speaker identification**:\n\n**Video:** A short interview with a software engineer.\n\n```\n00:00:00 - Interviewer: Welcome, Sarah, thanks for joining us today.\n\n00:00:03 - Sarah: Thanks for having me!\n\n00:00:05 - Interviewer: So, tell us a little bit about your role as a software engineer.\n\n00:00:10 - Sarah: Sure.  I work on the front-end development team.  We're responsible for building the user interface for our web application.  So, basically, what the users see and interact with.\n\n00:00:20 - Interviewer: And what are some of the challenges you face in that role?\n\n00:00:24 - Sarah: Well, keeping up with the latest technologies is always a challenge.  The web development landscape is constantly evolving, so we need to be constantly learning and adapting.  Also, ensuring a consistent user experience across different browsers and devices can be tricky.\n\n00:00:38 - Interviewer: That makes sense. What do you enjoy most about your job?\n\n00:00:42 - Sarah: I really enjoy the problem-solving aspect.  Taking a complex problem and breaking it down into smaller, manageable pieces, and then finding creative solutions.  And, of course, seeing users actually benefit from the features we build.\n\n00:00:54 - Interviewer: That's great. Any advice for aspiring software engineers?\n\n00:00:58 - Sarah: Definitely focus on building a strong foundation in fundamental concepts.  And don't be afraid to experiment and try new things.  The best way to learn is by doing.\n\n00:01:07 - Interviewer: Excellent advice. Thanks again for your time, Sarah.\n\n00:01:10 - Sarah: You're welcome!\n```\n\n**Key features of this example:**\n\n*   **Speaker Identification:**  Clearly identifies who is speaking (Interviewer and Sarah).\n*   **Timecoding:**  Timestamps (e.g., `00:00:00`) indicate the start time of each utterance.\n*   **Clean Verbatim:**  Filler words like \"um\" and \"uh\" have been removed for readability.  Minor pauses are also omitted.\n*   **Clear and Concise:**  The transcription is easy to read and understand.\n\nThis is just one example, and the specific format and level of detail will vary depending on the requirements of the project.  A verbatim transcription would include all the \"ums\" and \"ahs,\" while a more heavily edited transcription might rephrase sentences for even greater clarity."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------- end of response --------------------------------\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "What is the terminology used for video transcriptions?\n",
        "Please show a typical output example.\n",
        "\"\"\"\n",
        "generate_content(prompt, show_as=ShowAs.MARKDOWN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpolKTaThj36"
      },
      "source": [
        "### ðŸ“ Tabular extraction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KTjxDPoqoN2"
      },
      "source": [
        "So far, we've seen the following:\n",
        "\n",
        "- We didn't manage to get the full transcription with identified speakers all at once\n",
        "- Order matters (because a generated token influences the probabilities for subsequent tokens)\n",
        "\n",
        "To tackle our challenge, we need Gemini to infer from the following multimodal information:\n",
        "\n",
        "- text (our instructions + what may be written in the video)\n",
        "- audio cues (everything said or audible in the video's audio)\n",
        "- visual cues (everything visible in the video)\n",
        "- time (when things happen)\n",
        "\n",
        "That is quite a mixture of information types!\n",
        "\n",
        "As video transcription is a data extraction use case, if we think about the final result as a database, our final goal can be seen as the generation of two related tables (transcripts and speakers). If we write it down, our initial three sub-problems now look decoupled:\n",
        "\n",
        "![transcripts and speakers tables](https://storage.googleapis.com/github-repo/generative-ai/gemini/use-cases/video-analysis/multimodal_video_transcription/tabular-extraction-1.png)\n",
        "\n",
        "> ðŸ’¡ In computer science, data decoupling enhances data locality, often yielding improved performance across areas such as cache utilization, data access, semantic understanding, or system maintenance. Within the LLM Transformer architecture, core performance relies heavily on the attention mechanism. Nonetheless, the attention pool is finite and tokens compete for attention. Researchers sometimes refer to \"attention dilution\" for long-context, million-token-scale benchmarks. While we cannot directly debug LLMs as users, intuitively, data decoupling may improve the model's focus, leading to a better attention span.\n",
        "\n",
        "Since Gemini is extremely good with patterns, it can automatically generate identifiers to link our tables. In addition, since we eventually want an automated workflow, we can start reasoning in terms of data and fields:\n",
        "\n",
        "![transcripts and speakers tables with id](https://storage.googleapis.com/github-repo/generative-ai/gemini/use-cases/video-analysis/multimodal_video_transcription/tabular-extraction-2.png)\n",
        "\n",
        "Let's call this approach \"tabular extraction\", split our instructions into two tasks (tables), still in a single request, and arrange them in a meaningful orderâ€¦\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUSUffpYhj36"
      },
      "source": [
        "### ðŸ’¬ Transcripts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fw5nQJ9qoN2"
      },
      "source": [
        "First of all, let's focus on getting the audio transcripts:\n",
        "\n",
        "- Gemini has proven to be natively good at audio transcription\n",
        "- This requires less inference than image analysis\n",
        "- It is central and independent information\n",
        "\n",
        "> ðŸ’¡ Generating an output that starts with correct answers should help to achieve an overall correct output.\n",
        "\n",
        "We've also seen what a typical transcription entry can look like:\n",
        "\n",
        "`00:02 speaker_1: Welcome!`\n",
        "\n",
        "But, right away, there can be some ambiguities in our multimodal use case:\n",
        "\n",
        "- What is a speaker?\n",
        "- Is it someone we see/hear?\n",
        "- What if the person visible in the video is not the one speaking?\n",
        "- What if the person speaking is never seen in the video?\n",
        "\n",
        "How do we unconsciously identify who is speaking in a video?\n",
        "\n",
        "- First, probably by identifying the different voices on the fly?\n",
        "- Then, probably by consolidating additional audio and visual cues?\n",
        "\n",
        "Can Gemini understand voice characteristics?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "BZpvtyEZqoN2",
        "outputId": "a7e48e6d-89d7-4a59-ed5b-7a2896f70eaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------- GDM_PODCAST_TRAILER_PT59S / gemini-2.0-flash -----------------\n",
            "Input tokens   :    16,730\n",
            "Output tokens  :       145\n",
            "------------------------------ start of response -------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here's a breakdown of the audible characteristics from the video's audio:\n\n*   **Voice Tones:** The tones are generally conversational, enthusiastic, and curious. There are also moments of seriousness and excitement.\n*   **Voice Pitches:** There's a range of pitches, from higher-pitched female voices to lower-pitched male voices.\n*   **Languages:** The primary language is English.\n*   **Accents:** There are various accents, including British, American, and possibly others that are harder to pinpoint without more context.\n*   **Speaking Styles:** The speaking styles vary from formal (like a host introducing a podcast) to more casual and conversational (like interviews)."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------- end of response --------------------------------\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Using only the video's audio, list the following audible characteristics:\n",
        "- Voice tones\n",
        "- Voice pitches\n",
        "- Languages\n",
        "- Accents\n",
        "- Speaking styles\n",
        "\"\"\"\n",
        "video = TestVideo.GDM_PODCAST_TRAILER_PT59S\n",
        "\n",
        "generate_content(prompt, video, show_as=ShowAs.MARKDOWN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRy_zzFHqoN3"
      },
      "source": [
        "What about a French video?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "x6bb7r_SqoN3",
        "outputId": "047b73fa-fcfa-4712-d146-786d66e73f54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------- BRUT_FR_DOGS_WATER_LEAK_PT8M28S / gemini-2.0-flash --------------\n",
            "Input tokens   :   144,055\n",
            "Output tokens  :        71\n",
            "------------------------------ start of response -------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here is a list of the audible characteristics of the video's audio:\n\n- **Voice tones:** Conversational, informative, enthusiastic, serious, humorous\n- **Voice pitches:** Varying, from low to high\n- **Languages:** French\n- **Accents:** Standard French\n- **Speaking styles:** Clear, articulate, professional, casual"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------- end of response --------------------------------\n"
          ]
        }
      ],
      "source": [
        "video = TestVideo.BRUT_FR_DOGS_WATER_LEAK_PT8M28S\n",
        "\n",
        "generate_content(prompt, video, show_as=ShowAs.MARKDOWN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19qIqFbdqoN3"
      },
      "source": [
        "> âš ï¸ We have to be cautious here: responses can consolidate multimodal information or even general knowledge. For example, if a person is famous, their name is most likely part of the LLM's knowledge. If they are known to be from the UK, a possible inference is that they have a British accent. This is why we made our prompt more specific by including \"using only the video's audio\".\n",
        "\n",
        "> ðŸ’¡ If you conduct more tests, for example on private audio files (i.e., not part of common knowledge and with no additional visual cues), you'll see that Gemini's audio tokenizer performs exceptionally well and extracts semantic speech information!\n",
        "\n",
        "After a few iterations, we can arrive at a transcription prompt focusing on the audio and voices:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qWDIQMNmqoN3",
        "outputId": "ab7c7fa1-310b-4318-a5a6-1d8343560e89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------- GDM_PODCAST_TRAILER_PT59S / gemini-2.0-flash -----------------\n",
            "Input tokens   :    16,800\n",
            "Output tokens  :       728\n",
            "------------------------------ start of response -------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```json\n[\n  {\n    \"start\": \"00:00\",\n    \"text\": \"Do I have to call you Sir Demis now?\",\n    \"voice\": 1\n  },\n  {\n    \"start\": \"00:01\",\n    \"text\": \"Oh, you don't. Absolutely not.\",\n    \"voice\": 2\n  },\n  {\n    \"start\": \"00:03\",\n    \"text\": \"Welcome to Google DeepMind the podcast with me, your host Professor Hannah Fry.\",\n    \"voice\": 1\n  },\n  {\n    \"start\": \"00:06\",\n    \"text\": \"We want to take you to the heart of where these ideas are coming from.\",\n    \"voice\": 1\n  },\n  {\n    \"start\": \"00:12\",\n    \"text\": \"We want to introduce you to the people who are leading the design of our collective future.\",\n    \"voice\": 1\n  },\n  {\n    \"start\": \"00:19\",\n    \"text\": \"Getting the safety right is probably, I'd say, one of the most important challenges of our time.\",\n    \"voice\": 3\n  },\n  {\n    \"start\": \"00:25\",\n    \"text\": \"I want safe and capable.\",\n    \"voice\": 3\n  },\n  {\n    \"start\": \"00:27\",\n    \"text\": \"I want a bridge that will not collapse.\",\n    \"voice\": 3\n  },\n  {\n    \"start\": \"00:30\",\n    \"text\": \"just give these scientists a superpower that they had not imagined earlier.\",\n    \"voice\": 4\n  },\n  {\n    \"start\": \"00:34\",\n    \"text\": \"autonomous vehicles. It's hard to fathom that when you're working on a search engine.\",\n    \"voice\": 5\n  },\n  {\n    \"start\": \"00:38\",\n    \"text\": \"We may see entirely new genre or entirely new forms of art come up.\",\n    \"voice\": 6\n  },\n  {\n    \"start\": \"00:42\",\n    \"text\": \"There may be a new word that is not music, painting, photography, moviemaking, and that AI will have helped us create it.\",\n    \"voice\": 6\n  },\n  {\n    \"start\": \"00:48\",\n    \"text\": \"You really want AGI to be able to peer into the mysteries of the universe.\",\n    \"voice\": 1\n  },\n  {\n    \"start\": \"00:51\",\n    \"text\": \"Yes, quantum mechanics, string theory, well, and the nature of reality.\",\n    \"voice\": 2\n  },\n  {\n    \"start\": \"00:55\",\n    \"text\": \"Ow.\",\n    \"voice\": 1\n  },\n  {\n    \"start\": \"00:56\",\n    \"text\": \"The magic of AI.\",\n    \"voice\": 6\n  }\n]\n```"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------- end of response --------------------------------\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Task:\n",
        "- Watch the video and listen carefully to the audio.\n",
        "- Identify each unique voice using a `voice` ID (1, 2, 3, etc.).\n",
        "- Transcribe the video's audio verbatim with voice diarization.\n",
        "- Include the `start` timecode (MM:SS) for each speech segment.\n",
        "- Output a JSON array where each object has the following fields:\n",
        "  - `start`\n",
        "  - `text`\n",
        "  - `voice`\n",
        "\"\"\"\n",
        "video = TestVideo.GDM_PODCAST_TRAILER_PT59S\n",
        "\n",
        "generate_content(prompt, video, show_as=ShowAs.MARKDOWN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxiB4dK3qoN3"
      },
      "source": [
        "This is looking good! And if you test these instructions on more complex videos, you'll get similarly promising results.\n",
        "\n",
        "Notice how the prompt reuses cherry-picked terms from the terminology previously provided by Gemini, while aiming for precision and concision:\n",
        "\n",
        "- `verbatim` is unambiguous (unlike \"spoken words\")\n",
        "- `1, 2, 3, etc.` is an ellipsis (Gemini can infer the pattern)\n",
        "- `timecode` is specific (`timestamp` has more meanings)\n",
        "- `MM:SS` clarifies the timecode format\n",
        "\n",
        "> ðŸ’¡ Gemini 2.0 was trained to understand the specific `MM:SS` timecode format. Gemini 2.5 also supports the `H:MM:SS` format for longer videos. For the latest updates, refer to the [video understanding documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/video-understanding).\n",
        "\n",
        "We're halfway there. Let's complete our database generation with a second taskâ€¦\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjSoAYomqoN3"
      },
      "source": [
        "### ðŸ§‘ Speakers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4JRfMpOqoN3"
      },
      "source": [
        "The second task is pretty straightforward: we want to extract speaker information into a second table. The two tables are logically linked by the voice ID.\n",
        "\n",
        "After a few iterations, we can reach a two-task prompt like the following:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "52ysk17GqoN3",
        "outputId": "6b2c6113-4da9-4d77-f060-286b0920ebc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------- GDM_PODCAST_TRAILER_PT59S / gemini-2.0-flash -----------------\n",
            "Input tokens   :    16,943\n",
            "Output tokens  :       855\n",
            "------------------------------ start of response -------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here are the JSON objects containing the transcripts and speaker information for the provided video:\n\n```json\n{\n  \"task1_transcripts\": [\n    {\n      \"start\": \"00:00\",\n      \"text\": \"Do I have to call you Sir Demis now?\",\n      \"voice\": 1\n    },\n    {\n      \"start\": \"00:01\",\n      \"text\": \"Oh, you don't. Absolutely not.\",\n      \"voice\": 2\n    },\n    {\n      \"start\": \"00:03\",\n      \"text\": \"Welcome to Google DeepMind the podcast with me, your host Professor Hannah Fry.\",\n      \"voice\": 1\n    },\n    {\n      \"start\": \"00:06\",\n      \"text\": \"We want to take you to the heart of where these ideas are coming from. We want to introduce you to the people who are leading the design of our collective future.\",\n      \"voice\": 1\n    },\n    {\n      \"start\": \"00:19\",\n      \"text\": \"Getting the safety right is probably, I'd say, one of the most important challenges of our time. I want safe and capable.\",\n      \"voice\": 3\n    },\n    {\n      \"start\": \"00:27\",\n      \"text\": \"I want a bridge that will not collapse.\",\n      \"voice\": 3\n    },\n    {\n      \"start\": \"00:30\",\n      \"text\": \"just give these scientists a superpower that they had not imagined earlier.\",\n      \"voice\": 4\n    },\n    {\n      \"start\": \"00:34\",\n      \"text\": \"autonomous vehicles. It's hard to fathom that when you're working on a search engine.\",\n      \"voice\": 5\n    },\n    {\n      \"start\": \"00:38\",\n      \"text\": \"We may see entirely new genre or entirely new forms of art come up. There may be a new word that is not music, painting, photography, movie making, and that AI will have helped us create it.\",\n      \"voice\": 6\n    },\n    {\n      \"start\": \"00:48\",\n      \"text\": \"You really want AGI to be able to peer into the mysteries of the universe.\",\n      \"voice\": 1\n    },\n    {\n      \"start\": \"00:51\",\n      \"text\": \"Yes, quantum mechanics, string theory, well, and the nature of reality.\",\n      \"voice\": 2\n    },\n    {\n      \"start\": \"00:55\",\n      \"text\": \"Ow.\",\n      \"voice\": 1\n    },\n    {\n      \"start\": \"00:56\",\n      \"text\": \"the magic of AI.\",\n      \"voice\": 6\n    }\n  ],\n  \"task2_speakers\": [\n    {\n      \"voice\": 1,\n      \"name\": \"Professor Hannah Fry (red hair, green jumpsuit)\"\n    },\n    {\n      \"voice\": 2,\n      \"name\": \"Demis Hassabis (bald, glasses)\"\n    },\n    {\n      \"voice\": 3,\n      \"name\": \"Anca Dragan (brown hair, orange jacket)\"\n    },\n    {\n      \"voice\": 4,\n      \"name\": \"Pushmeet Kohli (beard, turban)\"\n    },\n    {\n      \"voice\": 5,\n      \"name\": \"Jeff Dean (short hair, green shirt)\"\n    },\n    {\n      \"voice\": 6,\n      \"name\": \"Douglas Eck (gray hair, glasses)\"\n    }\n  ]\n}\n```"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------- end of response --------------------------------\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Generate a JSON object with keys `task1_transcripts` and `task2_speakers` for the following tasks.\n",
        "\n",
        "**Task 1 - Transcripts**\n",
        "\n",
        "- Watch the video and listen carefully to the audio.\n",
        "- Identify each unique voice using a `voice` ID (1, 2, 3, etc.).\n",
        "- Transcribe the video's audio verbatim with voice diarization.\n",
        "- Include the `start` timecode (MM:SS) for each speech segment.\n",
        "- Output a JSON array where each object has the following fields:\n",
        "  - `start`\n",
        "  - `text`\n",
        "  - `voice`\n",
        "\n",
        "**Task 2 - Speakers**\n",
        "\n",
        "- For each `voice` ID from Task 1, extract information about the corresponding speaker.\n",
        "- Use visual and audio cues.\n",
        "- If a speaker's name cannot be found, use a question mark (`?`) as the value.\n",
        "- Output a JSON array where each object has the following fields:\n",
        "  - `voice`\n",
        "  - `name`\n",
        "\n",
        "\n",
        "Also, after finishing those, using voice clues assign each speaker how do they look like in parantheses.\n",
        "JSON:\n",
        "\"\"\"\n",
        "video = TestVideo.GDM_PODCAST_TRAILER_PT59S\n",
        "\n",
        "generate_content(prompt, video, show_as=ShowAs.MARKDOWN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGslUhnAqoN3"
      },
      "source": [
        "Test this prompt on more complex videos: it's still looking good!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i68a20ekqoN4"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸš€ Finalization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slrAAtrwhj37"
      },
      "source": [
        "### ðŸ§© Structured output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7hHAYumqoN4"
      },
      "source": [
        "We've iterated towards a precise and concise prompt. Now, we can focus on Gemini's response:\n",
        "\n",
        "- The response is plain text containing fenced code blocks\n",
        "- Instead, we'd like a structured output, to receive consistently formatted responses\n",
        "- Ideally, we'd also like to avoid having to parse the response, which can be a maintenance burden\n",
        "\n",
        "Getting structured outputs is an LLM feature also called \"controlled generation\". Since we've already crafted our prompt in terms of data tables and JSON fields, this is now a formality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PlFOzZ5GLM7"
      },
      "source": [
        "In our request, we can add the following parameters:\n",
        "\n",
        "- `response_mime_type=\"application/json\"`\n",
        "- `response_schema=\"YOUR_JSON_SCHEMA\"` ([docs](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output#fields))\n",
        "\n",
        "In Python, this gets even easier:\n",
        "\n",
        "- Use the `pydantic` library\n",
        "- Reflect your output structure with classes derived from `pydantic.BaseModel`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80OSaujFGcC4"
      },
      "source": [
        "We can simplify the prompt by removing the output specification parts:\n",
        "\n",
        "```markdown\n",
        "Generate a JSON object with keys `task1_transcripts` and `task2_speakers` for the following tasks.\n",
        "â€¦\n",
        "- Output a JSON array where each object has the following fields:\n",
        "  - `start`\n",
        "  - `text`\n",
        "  - `voice`\n",
        "â€¦\n",
        "- Output a JSON array where each object has the following fields:\n",
        "  - `voice`\n",
        "  - `name`\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL4_hgRPBxUn"
      },
      "source": [
        "â€¦ to move them to matching Python classes instead:\n",
        "\n",
        "```python\n",
        "import pydantic\n",
        "\n",
        "class Transcript(pydantic.BaseModel):\n",
        "    start: str\n",
        "    text: str\n",
        "    voice: int\n",
        "\n",
        "class Speaker(pydantic.BaseModel):\n",
        "    voice: int\n",
        "    name: str\n",
        "\n",
        "class VideoTranscription(pydantic.BaseModel):\n",
        "    task1_transcripts: list[Transcript] = pydantic.Field(default_factory=list)\n",
        "    task2_speakers: list[Speaker] = pydantic.Field(default_factory=list)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jji8A6iU2lR"
      },
      "source": [
        "â€¦ and request a structured response:\n",
        "\n",
        "```python\n",
        "response = client.models.generate_content(\n",
        "    # â€¦\n",
        "    config=GenerateContentConfig(\n",
        "        # â€¦\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=VideoTranscription,\n",
        "        # â€¦\n",
        "    ),\n",
        ")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJyFqav5U_Iz"
      },
      "source": [
        "Finally, retrieving the objects from the response is also direct:\n",
        "\n",
        "```python\n",
        "if isinstance(response.parsed, VideoTranscription):\n",
        "    video_transcription = response.parsed\n",
        "else:\n",
        "    video_transcription = VideoTranscription()  # Empty transcription\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqXL2ZqxVIUL"
      },
      "source": [
        "The interesting aspects of this approach are the following:\n",
        "\n",
        "- The prompt focuses on the logic and the classes focus on the output\n",
        "- It's easier to update and maintain typed classes\n",
        "- The JSON schema is automatically generated by the Gen AI SDK from the class provided in `response_schema` and dispatched to Gemini\n",
        "- The response is automatically parsed by the Gen AI SDK and deserialized into the corresponding Python objects\n",
        "\n",
        "> âš ï¸ If you keep output specifications in your prompt, ensure there are no contradictions between the prompt and the schema (e.g., same field names and order), as this can negatively impact the quality of the responses.\n",
        "\n",
        "> ðŸ’¡ It's possible to have more structural information directly in the schema (e.g., detailed field definitions). See [Controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_WXb2Lmhj37"
      },
      "source": [
        "### âœ¨ Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaCqQHfhhj37"
      },
      "source": [
        "Let's finalize our code. In addition, now that we have a stable prompt, we can even enrich our solution to extract each speaker's `company`, `position`, and `role_in_video`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RePBxIKqqoN4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "import pydantic\n",
        "from google.genai.types import MediaResolution, ThinkingConfig\n",
        "\n",
        "SamplingFrameRate = float\n",
        "\n",
        "VIDEO_TRANSCRIPTION_PROMPT = \"\"\"\n",
        "**Task 1 - Transcripts**\n",
        "\n",
        "- Watch the video and listen carefully to the audio.\n",
        "- Identify each unique voice using a `voice` ID (1, 2, 3, etc.).\n",
        "- Transcribe the video's audio verbatim with voice diarization.\n",
        "- Include the `start` timecode ({timecode_spec}) for each speech segment.\n",
        "\n",
        "**Task 2 - Speakers**\n",
        "\n",
        "- For each `voice` ID from Task 1, extract information about the corresponding speaker.\n",
        "- Use visual and audio cues.\n",
        "- If a piece of information cannot be found, use a question mark (`?`) as the value.\n",
        "\"\"\"\n",
        "NOT_FOUND = \"?\"\n",
        "\n",
        "\n",
        "class Transcript(pydantic.BaseModel):\n",
        "    start: str\n",
        "    text: str\n",
        "    voice: int\n",
        "\n",
        "\n",
        "class Speaker(pydantic.BaseModel):\n",
        "    voice: int\n",
        "    name: str\n",
        "    company: str\n",
        "    position: str\n",
        "    role_in_video: str\n",
        "\n",
        "\n",
        "class VideoTranscription(pydantic.BaseModel):\n",
        "    task1_transcripts: list[Transcript] = pydantic.Field(default_factory=list)\n",
        "    task2_speakers: list[Speaker] = pydantic.Field(default_factory=list)\n",
        "\n",
        "\n",
        "def get_generate_content_config(model: Model, video: Video) -> GenerateContentConfig:\n",
        "    media_resolution = get_media_resolution_for_video(video)\n",
        "    thinking_config = get_thinking_config(model)\n",
        "\n",
        "    return GenerateContentConfig(\n",
        "        temperature=DEFAULT_CONFIG.temperature,\n",
        "        top_p=DEFAULT_CONFIG.top_p,\n",
        "        seed=DEFAULT_CONFIG.seed,\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=VideoTranscription,\n",
        "        media_resolution=media_resolution,\n",
        "        thinking_config=thinking_config,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_video_duration(video: Video) -> timedelta | None:\n",
        "    # For testing purposes, video duration is statically specified in the enum name\n",
        "    # Suffix (ISO 8601 based): _PT[<h>H][<m>M][<s>S]\n",
        "    # For production,\n",
        "    # - fetch durations dynamically or store them separately\n",
        "    # - take into account video VideoMetadata.start_offset & VideoMetadata.end_offset\n",
        "    regex = r\"_PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?$\"\n",
        "    if not (match := re.search(regex, video.name)):\n",
        "        print(f\"âš ï¸ No duration info in {video.name}. Will use defaults.\")\n",
        "        return None\n",
        "\n",
        "    h_str, m_str, s_str = match.groups()\n",
        "    return timedelta(\n",
        "        hours=int(h_str) if h_str is not None else 0,\n",
        "        minutes=int(m_str) if m_str is not None else 0,\n",
        "        seconds=int(s_str) if s_str is not None else 0,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_media_resolution_for_video(video: Video) -> MediaResolution | None:\n",
        "    if not (video_duration := get_video_duration(video)):\n",
        "        return None  # Default\n",
        "\n",
        "    # For testing purposes, this is based on video duration, as our short videos tend to be more detailed\n",
        "    less_than_five_minutes = video_duration < timedelta(minutes=5)\n",
        "    if less_than_five_minutes:\n",
        "        media_resolution = MediaResolution.MEDIA_RESOLUTION_MEDIUM\n",
        "    else:\n",
        "        media_resolution = MediaResolution.MEDIA_RESOLUTION_LOW\n",
        "\n",
        "    return media_resolution\n",
        "\n",
        "\n",
        "def get_sampling_frame_rate_for_video(video: Video) -> SamplingFrameRate | None:\n",
        "    sampling_frame_rate = None  # Default (1 FPS for current models)\n",
        "\n",
        "    # [Optional] Define a custom FPS: 0.0 < sampling_frame_rate <= 24.0\n",
        "\n",
        "    return sampling_frame_rate\n",
        "\n",
        "\n",
        "def get_timecode_spec_for_model_and_video(model: Model, video: Video) -> str:\n",
        "    timecode_spec = \"MM:SS\"  # Default\n",
        "\n",
        "    match model:\n",
        "        case Model.GEMINI_2_0_FLASH:  # Supports MM:SS\n",
        "            pass\n",
        "        case Model.GEMINI_2_5_FLASH | Model.GEMINI_2_5_PRO:  # Support MM:SS and H:MM:SS\n",
        "            duration = get_video_duration(video)\n",
        "            one_hour_or_more = duration is not None and timedelta(hours=1) <= duration\n",
        "            if one_hour_or_more:\n",
        "                timecode_spec = \"MM:SS or H:MM:SS\"\n",
        "        case _:\n",
        "            assert False, \"Add timecode format for new model\"\n",
        "\n",
        "    return timecode_spec\n",
        "\n",
        "\n",
        "def get_thinking_config(model: Model) -> ThinkingConfig | None:\n",
        "    # Examples of thinking configurations (Gemini 2.5 models)\n",
        "    match model:\n",
        "        case Model.GEMINI_2_5_FLASH:  # Thinking disabled\n",
        "            return ThinkingConfig(thinking_budget=0, include_thoughts=False)\n",
        "        case Model.GEMINI_2_5_PRO:  # Minimum thinking budget and no summarized thoughts\n",
        "            return ThinkingConfig(thinking_budget=128, include_thoughts=False)\n",
        "        case _:\n",
        "            return None  # Default\n",
        "\n",
        "\n",
        "def get_video_transcription_from_response(\n",
        "    response: GenerateContentResponse,\n",
        ") -> VideoTranscription:\n",
        "    if not isinstance(response.parsed, VideoTranscription):\n",
        "        print(\"âŒ Could not parse the JSON response\")\n",
        "        return VideoTranscription()  # Empty transcription\n",
        "\n",
        "    return response.parsed\n",
        "\n",
        "\n",
        "def get_video_transcription(\n",
        "    video: Video,\n",
        "    video_segment: VideoSegment | None = None,\n",
        "    fps: float | None = None,\n",
        "    prompt: str | None = None,\n",
        "    model: Model | None = None,\n",
        ") -> VideoTranscription:\n",
        "    model = model or Model.DEFAULT\n",
        "    model_id = model.value\n",
        "\n",
        "    fps = fps or get_sampling_frame_rate_for_video(video)\n",
        "    video_part = get_video_part(video, video_segment, fps)\n",
        "    if not video_part:  # Unsupported source, return an empty transcription\n",
        "        return VideoTranscription()\n",
        "    if prompt is None:\n",
        "        timecode_spec = get_timecode_spec_for_model_and_video(model, video)\n",
        "        prompt = VIDEO_TRANSCRIPTION_PROMPT.format(timecode_spec=timecode_spec)\n",
        "    contents = [video_part, prompt.strip()]\n",
        "\n",
        "    config = get_generate_content_config(model, video)\n",
        "\n",
        "    print(f\" {video.name} / {model_id} \".center(80, \"-\"))\n",
        "    response = None\n",
        "    for attempt in get_retrier():\n",
        "        with attempt:\n",
        "            response = client.models.generate_content(\n",
        "                model=model_id,\n",
        "                contents=contents,\n",
        "                config=config,\n",
        "            )\n",
        "            display_response_info(response)\n",
        "\n",
        "    assert isinstance(response, GenerateContentResponse)\n",
        "    return get_video_transcription_from_response(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5s3EKrKqoN4"
      },
      "source": [
        "Test it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "j0_pIQVHqoN4",
        "outputId": "c8d890fc-b56f-40ac-a5ec-d791b36c10f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------- GDM_PODCAST_TRAILER_PT59S / gemini-2.0-flash -----------------\n",
            "Input tokens   :    16,920\n",
            "Output tokens  :     1,021\n",
            "--------------------------------------------------------------------------------\n",
            "Transcripts :  14\n",
            "Speakers    :   6\n",
            "- voice=1 name='Professor Hannah Fry' company='Google DeepMind' position='Host' role_in_video='host'\n",
            "- voice=2 name='Demis Hassabis' company='Google DeepMind' position='Co-Founder & CEO' role_in_video='interviewee'\n",
            "- voice=3 name='Anca Dragan' company='?' position='Director, AI Safety & Alignment' role_in_video='interviewee'\n",
            "- voice=4 name='Pushmeet Kohli' company='?' position='VP Science & Strategic Initiatives' role_in_video='interviewee'\n",
            "- voice=5 name='Jeff Dean' company='?' position='Chief Scientist' role_in_video='interviewee'\n",
            "- voice=6 name='Douglas Eck' company='?' position='Senior Research Director' role_in_video='interviewee'\n"
          ]
        }
      ],
      "source": [
        "def test_structured_video_transcription(video: Video) -> None:\n",
        "    transcription = get_video_transcription(video)\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Transcripts : {len(transcription.task1_transcripts):3d}\")\n",
        "    print(f\"Speakers    : {len(transcription.task2_speakers):3d}\")\n",
        "    for speaker in transcription.task2_speakers:\n",
        "        print(f\"- {speaker}\")\n",
        "\n",
        "\n",
        "test_structured_video_transcription(TestVideo.GDM_PODCAST_TRAILER_PT59S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa1MrRKsqoN4"
      },
      "source": [
        "### ðŸ“Š Data visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcGct03AqoN4"
      },
      "source": [
        "We started prototyping in natural language, crafted a prompt, and generated a structured output. Since reading raw data can be cumbersome, we can now present video transcriptions in a more visually appealing way.\n",
        "\n",
        "Here's a possible orchestrator function:\n",
        "\n",
        "```python\n",
        "def transcribe_video(video: Video, â€¦) -> None:\n",
        "    display_video(video)\n",
        "    transcription = get_video_transcription(video, â€¦)\n",
        "    display_speakers(transcription)\n",
        "    display_transcripts(transcription)\n",
        "```\n",
        "\n",
        "Let's add some data visualization functionsâ€¦\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "W5u23XdeqoN4"
      },
      "outputs": [],
      "source": [
        "# @title {display-mode: \"form\"}\n",
        "\n",
        "import itertools\n",
        "from collections.abc import Callable, Iterator\n",
        "\n",
        "from pandas import DataFrame, Series\n",
        "from pandas.io.formats.style import Styler\n",
        "from pandas.io.formats.style_render import CSSDict\n",
        "\n",
        "BGCOLOR_COLUMN = \"bg_color\"  # Hidden column to store row background colors\n",
        "\n",
        "\n",
        "def yield_known_speaker_color() -> Iterator[str]:\n",
        "    PAL_40 = (\"#669DF6\", \"#EE675C\", \"#FCC934\", \"#5BB974\")\n",
        "    PAL_30 = (\"#8AB4F8\", \"#F28B82\", \"#FDD663\", \"#81C995\")\n",
        "    PAL_20 = (\"#AECBFA\", \"#F6AEA9\", \"#FDE293\", \"#A8DAB5\")\n",
        "    PAL_10 = (\"#D2E3FC\", \"#FAD2CF\", \"#FEEFC3\", \"#CEEAD6\")\n",
        "    PAL_05 = (\"#E8F0FE\", \"#FCE8E6\", \"#FEF7E0\", \"#E6F4EA\")\n",
        "    return itertools.cycle([*PAL_40, *PAL_30, *PAL_20, *PAL_10, *PAL_05])\n",
        "\n",
        "\n",
        "def yield_unknown_speaker_color() -> Iterator[str]:\n",
        "    GRAYS = [\"#80868B\", \"#9AA0A6\", \"#BDC1C6\", \"#DADCE0\", \"#E8EAED\", \"#F1F3F4\"]\n",
        "    return itertools.cycle(GRAYS)\n",
        "\n",
        "\n",
        "def get_color_for_voice_mapping(speakers: list[Speaker]) -> dict[int, str]:\n",
        "    known_speaker_color = yield_known_speaker_color()\n",
        "    unknown_speaker_color = yield_unknown_speaker_color()\n",
        "\n",
        "    mapping: dict[int, str] = {}\n",
        "    for speaker in speakers:\n",
        "        if speaker.name != NOT_FOUND:\n",
        "            color = next(known_speaker_color)\n",
        "        else:\n",
        "            color = next(unknown_speaker_color)\n",
        "        mapping[speaker.voice] = color\n",
        "\n",
        "    return mapping\n",
        "\n",
        "\n",
        "def get_table_styler(df: DataFrame) -> Styler:\n",
        "    def join_styles(styles: list[str]) -> str:\n",
        "        return \";\".join(styles)\n",
        "\n",
        "    table_css = [\n",
        "        \"color: #202124\",\n",
        "        \"background-color: #BDC1C6\",\n",
        "        \"border: 0\",\n",
        "        \"border-radius: 0.5rem\",\n",
        "        \"border-spacing: 0px\",\n",
        "        \"outline: 0.5rem solid #BDC1C6\",\n",
        "        \"margin: 1rem 0.5rem\",\n",
        "    ]\n",
        "    th_css = [\"background-color: #E8EAED\"]\n",
        "    th_td_css = [\"text-align:left\", \"padding: 0.25rem 1rem\"]\n",
        "    table_styles = [\n",
        "        CSSDict(selector=\"\", props=join_styles(table_css)),\n",
        "        CSSDict(selector=\"th\", props=join_styles(th_css)),\n",
        "        CSSDict(selector=\"th,td\", props=join_styles(th_td_css)),\n",
        "    ]\n",
        "\n",
        "    return df.style.set_table_styles(table_styles).hide()\n",
        "\n",
        "\n",
        "def change_row_bgcolor(row: Series) -> list[str]:\n",
        "    style = f\"background-color:{row[BGCOLOR_COLUMN]}\"\n",
        "    return [style] * len(row)\n",
        "\n",
        "\n",
        "def display_table(yield_rows: Callable[[], Iterator[list[str]]]) -> None:\n",
        "    data = yield_rows()\n",
        "    df = DataFrame(columns=next(data), data=data)\n",
        "    styler = get_table_styler(df)\n",
        "    styler.apply(change_row_bgcolor, axis=1)\n",
        "    styler.hide([BGCOLOR_COLUMN], axis=\"columns\")\n",
        "\n",
        "    html = styler.to_html()\n",
        "    IPython.display.display(IPython.display.HTML(html))\n",
        "\n",
        "\n",
        "def display_speakers(transcription: VideoTranscription) -> None:\n",
        "    def sanitize_field(s: str, symbol_if_unknown: str) -> str:\n",
        "        return symbol_if_unknown if s == NOT_FOUND else s\n",
        "\n",
        "    def yield_rows() -> Iterator[list[str]]:\n",
        "        yield [\"voice\", \"name\", \"company\", \"position\", \"role_in_video\", BGCOLOR_COLUMN]\n",
        "\n",
        "        color_for_voice = get_color_for_voice_mapping(transcription.task2_speakers)\n",
        "        for speaker in transcription.task2_speakers:\n",
        "            yield [\n",
        "                str(speaker.voice),\n",
        "                sanitize_field(speaker.name, NOT_FOUND),\n",
        "                sanitize_field(speaker.company, NOT_FOUND),\n",
        "                sanitize_field(speaker.position, NOT_FOUND),\n",
        "                sanitize_field(speaker.role_in_video, NOT_FOUND),\n",
        "                color_for_voice.get(speaker.voice, \"red\"),\n",
        "            ]\n",
        "\n",
        "    display_markdown(f\"### Speakers ({len(transcription.task2_speakers)})\")\n",
        "    display_table(yield_rows)\n",
        "\n",
        "\n",
        "def display_transcripts(transcription: VideoTranscription) -> None:\n",
        "    def yield_rows() -> Iterator[list[str]]:\n",
        "        yield [\"start\", \"speaker\", \"transcript\", BGCOLOR_COLUMN]\n",
        "\n",
        "        color_for_voice = get_color_for_voice_mapping(transcription.task2_speakers)\n",
        "        speaker_for_voice = {\n",
        "            speaker.voice: speaker for speaker in transcription.task2_speakers\n",
        "        }\n",
        "        previous_voice = None\n",
        "        for transcript in transcription.task1_transcripts:\n",
        "            current_voice = transcript.voice\n",
        "            speaker_label = \"\"\n",
        "            if speaker := speaker_for_voice.get(current_voice):\n",
        "                if speaker.name != NOT_FOUND:\n",
        "                    speaker_label = speaker.name\n",
        "                elif speaker.position != NOT_FOUND:\n",
        "                    speaker_label = f\"[voice {current_voice}][{speaker.position}]\"\n",
        "                elif speaker.role_in_video != NOT_FOUND:\n",
        "                    speaker_label = f\"[voice {current_voice}][{speaker.role_in_video}]\"\n",
        "            if not speaker_label:\n",
        "                speaker_label = f\"[voice {current_voice}]\"\n",
        "            yield [\n",
        "                transcript.start,\n",
        "                speaker_label if current_voice != previous_voice else '\"',\n",
        "                transcript.text,\n",
        "                color_for_voice.get(current_voice, \"red\"),\n",
        "            ]\n",
        "            previous_voice = current_voice\n",
        "\n",
        "    display_markdown(f\"### Transcripts ({len(transcription.task1_transcripts)})\")\n",
        "    display_table(yield_rows)\n",
        "\n",
        "\n",
        "def transcribe_video(\n",
        "    video: Video,\n",
        "    video_segment: VideoSegment | None = None,\n",
        "    fps: float | None = None,\n",
        "    prompt: str | None = None,\n",
        "    model: Model | None = None,\n",
        ") -> None:\n",
        "    display_video(video)\n",
        "    transcription = get_video_transcription(video, video_segment, fps, prompt, model)\n",
        "    display_speakers(transcription)\n",
        "    display_transcripts(transcription)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSgqZuLDqoN4"
      },
      "source": [
        "---\n",
        "\n",
        "## âœ… Challenge completed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ91H4VWYMQk"
      },
      "source": [
        "### ðŸŽ¬ Short video\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj2LutM-hj39"
      },
      "source": [
        "This video is a trailer for the Google DeepMind podcast. It features a fast-paced montage of 6 interviews. The multimodal transcription is excellent:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "GPSuRE70Yhuk",
        "outputId": "1c4d1b33-db84-4a6c-e6d0-af4375b5c04d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Video ([source](https://www.youtube.com/watch?v=0pJn3g8dfwk))"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x78ad1ed234a0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"600\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/0pJn3g8dfwk\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "image/jpeg": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------- GDM_PODCAST_TRAILER_PT59S / gemini-2.0-flash -----------------\n",
            "Input tokens   :    16,920\n",
            "Output tokens  :       990\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Speakers (6)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_64017  {\n",
              "  color: #202124;\n",
              "  background-color: #BDC1C6;\n",
              "  border: 0;\n",
              "  border-radius: 0.5rem;\n",
              "  border-spacing: 0px;\n",
              "  outline: 0.5rem solid #BDC1C6;\n",
              "  margin: 1rem 0.5rem;\n",
              "}\n",
              "#T_64017 th {\n",
              "  background-color: #E8EAED;\n",
              "}\n",
              "#T_64017 th {\n",
              "  text-align: left;\n",
              "  padding: 0.25rem 1rem;\n",
              "}\n",
              "#T_64017 td {\n",
              "  text-align: left;\n",
              "  padding: 0.25rem 1rem;\n",
              "}\n",
              "#T_64017_row0_col0, #T_64017_row0_col1, #T_64017_row0_col2, #T_64017_row0_col3, #T_64017_row0_col4 {\n",
              "  background-color: #669DF6;\n",
              "}\n",
              "#T_64017_row1_col0, #T_64017_row1_col1, #T_64017_row1_col2, #T_64017_row1_col3, #T_64017_row1_col4 {\n",
              "  background-color: #EE675C;\n",
              "}\n",
              "#T_64017_row2_col0, #T_64017_row2_col1, #T_64017_row2_col2, #T_64017_row2_col3, #T_64017_row2_col4 {\n",
              "  background-color: #FCC934;\n",
              "}\n",
              "#T_64017_row3_col0, #T_64017_row3_col1, #T_64017_row3_col2, #T_64017_row3_col3, #T_64017_row3_col4 {\n",
              "  background-color: #5BB974;\n",
              "}\n",
              "#T_64017_row4_col0, #T_64017_row4_col1, #T_64017_row4_col2, #T_64017_row4_col3, #T_64017_row4_col4 {\n",
              "  background-color: #8AB4F8;\n",
              "}\n",
              "#T_64017_row5_col0, #T_64017_row5_col1, #T_64017_row5_col2, #T_64017_row5_col3, #T_64017_row5_col4 {\n",
              "  background-color: #F28B82;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_64017\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_64017_level0_col0\" class=\"col_heading level0 col0\" >voice</th>\n",
              "      <th id=\"T_64017_level0_col1\" class=\"col_heading level0 col1\" >name</th>\n",
              "      <th id=\"T_64017_level0_col2\" class=\"col_heading level0 col2\" >company</th>\n",
              "      <th id=\"T_64017_level0_col3\" class=\"col_heading level0 col3\" >position</th>\n",
              "      <th id=\"T_64017_level0_col4\" class=\"col_heading level0 col4\" >role_in_video</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_64017_row0_col0\" class=\"data row0 col0\" >1</td>\n",
              "      <td id=\"T_64017_row0_col1\" class=\"data row0 col1\" >Professor Hannah Fry</td>\n",
              "      <td id=\"T_64017_row0_col2\" class=\"data row0 col2\" >Google DeepMind</td>\n",
              "      <td id=\"T_64017_row0_col3\" class=\"data row0 col3\" >Host</td>\n",
              "      <td id=\"T_64017_row0_col4\" class=\"data row0 col4\" >Host</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_64017_row1_col0\" class=\"data row1 col0\" >2</td>\n",
              "      <td id=\"T_64017_row1_col1\" class=\"data row1 col1\" >Demis Hassabis</td>\n",
              "      <td id=\"T_64017_row1_col2\" class=\"data row1 col2\" >Google DeepMind</td>\n",
              "      <td id=\"T_64017_row1_col3\" class=\"data row1 col3\" >Co-Founder & CEO</td>\n",
              "      <td id=\"T_64017_row1_col4\" class=\"data row1 col4\" >Interviewee</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_64017_row2_col0\" class=\"data row2 col0\" >3</td>\n",
              "      <td id=\"T_64017_row2_col1\" class=\"data row2 col1\" >Anca Dragan</td>\n",
              "      <td id=\"T_64017_row2_col2\" class=\"data row2 col2\" >?</td>\n",
              "      <td id=\"T_64017_row2_col3\" class=\"data row2 col3\" >Director, AI Safety & Alignment</td>\n",
              "      <td id=\"T_64017_row2_col4\" class=\"data row2 col4\" >Interviewee</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_64017_row3_col0\" class=\"data row3 col0\" >4</td>\n",
              "      <td id=\"T_64017_row3_col1\" class=\"data row3 col1\" >Pushmeet Kohli</td>\n",
              "      <td id=\"T_64017_row3_col2\" class=\"data row3 col2\" >?</td>\n",
              "      <td id=\"T_64017_row3_col3\" class=\"data row3 col3\" >VP Science & Strategic Initiatives</td>\n",
              "      <td id=\"T_64017_row3_col4\" class=\"data row3 col4\" >Interviewee</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_64017_row4_col0\" class=\"data row4 col0\" >5</td>\n",
              "      <td id=\"T_64017_row4_col1\" class=\"data row4 col1\" >Jeff Dean</td>\n",
              "      <td id=\"T_64017_row4_col2\" class=\"data row4 col2\" >?</td>\n",
              "      <td id=\"T_64017_row4_col3\" class=\"data row4 col3\" >Chief Scientist</td>\n",
              "      <td id=\"T_64017_row4_col4\" class=\"data row4 col4\" >Interviewee</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_64017_row5_col0\" class=\"data row5 col0\" >6</td>\n",
              "      <td id=\"T_64017_row5_col1\" class=\"data row5 col1\" >Douglas Eck</td>\n",
              "      <td id=\"T_64017_row5_col2\" class=\"data row5 col2\" >?</td>\n",
              "      <td id=\"T_64017_row5_col3\" class=\"data row5 col3\" >Senior Research Director</td>\n",
              "      <td id=\"T_64017_row5_col4\" class=\"data row5 col4\" >Interviewee</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Transcripts (13)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_59c8c  {\n",
              "  color: #202124;\n",
              "  background-color: #BDC1C6;\n",
              "  border: 0;\n",
              "  border-radius: 0.5rem;\n",
              "  border-spacing: 0px;\n",
              "  outline: 0.5rem solid #BDC1C6;\n",
              "  margin: 1rem 0.5rem;\n",
              "}\n",
              "#T_59c8c th {\n",
              "  background-color: #E8EAED;\n",
              "}\n",
              "#T_59c8c th {\n",
              "  text-align: left;\n",
              "  padding: 0.25rem 1rem;\n",
              "}\n",
              "#T_59c8c td {\n",
              "  text-align: left;\n",
              "  padding: 0.25rem 1rem;\n",
              "}\n",
              "#T_59c8c_row0_col0, #T_59c8c_row0_col1, #T_59c8c_row0_col2, #T_59c8c_row2_col0, #T_59c8c_row2_col1, #T_59c8c_row2_col2, #T_59c8c_row3_col0, #T_59c8c_row3_col1, #T_59c8c_row3_col2, #T_59c8c_row9_col0, #T_59c8c_row9_col1, #T_59c8c_row9_col2, #T_59c8c_row11_col0, #T_59c8c_row11_col1, #T_59c8c_row11_col2 {\n",
              "  background-color: #669DF6;\n",
              "}\n",
              "#T_59c8c_row1_col0, #T_59c8c_row1_col1, #T_59c8c_row1_col2, #T_59c8c_row10_col0, #T_59c8c_row10_col1, #T_59c8c_row10_col2 {\n",
              "  background-color: #EE675C;\n",
              "}\n",
              "#T_59c8c_row4_col0, #T_59c8c_row4_col1, #T_59c8c_row4_col2, #T_59c8c_row5_col0, #T_59c8c_row5_col1, #T_59c8c_row5_col2 {\n",
              "  background-color: #FCC934;\n",
              "}\n",
              "#T_59c8c_row6_col0, #T_59c8c_row6_col1, #T_59c8c_row6_col2 {\n",
              "  background-color: #5BB974;\n",
              "}\n",
              "#T_59c8c_row7_col0, #T_59c8c_row7_col1, #T_59c8c_row7_col2 {\n",
              "  background-color: #8AB4F8;\n",
              "}\n",
              "#T_59c8c_row8_col0, #T_59c8c_row8_col1, #T_59c8c_row8_col2, #T_59c8c_row12_col0, #T_59c8c_row12_col1, #T_59c8c_row12_col2 {\n",
              "  background-color: #F28B82;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_59c8c\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_59c8c_level0_col0\" class=\"col_heading level0 col0\" >start</th>\n",
              "      <th id=\"T_59c8c_level0_col1\" class=\"col_heading level0 col1\" >speaker</th>\n",
              "      <th id=\"T_59c8c_level0_col2\" class=\"col_heading level0 col2\" >transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_59c8c_row0_col0\" class=\"data row0 col0\" >00:00</td>\n",
              "      <td id=\"T_59c8c_row0_col1\" class=\"data row0 col1\" >Professor Hannah Fry</td>\n",
              "      <td id=\"T_59c8c_row0_col2\" class=\"data row0 col2\" >Do I have to call you Sir Demis now?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_59c8c_row1_col0\" class=\"data row1 col0\" >00:01</td>\n",
              "      <td id=\"T_59c8c_row1_col1\" class=\"data row1 col1\" >Demis Hassabis</td>\n",
              "      <td id=\"T_59c8c_row1_col2\" class=\"data row1 col2\" >Oh, you don't. Absolutely not.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_59c8c_row2_col0\" class=\"data row2 col0\" >00:03</td>\n",
              "      <td id=\"T_59c8c_row2_col1\" class=\"data row2 col1\" >Professor Hannah Fry</td>\n",
              "      <td id=\"T_59c8c_row2_col2\" class=\"data row2 col2\" >Welcome to Google Deep Mind the podcast with me, your host, Professor Hannah Fry.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_59c8c_row3_col0\" class=\"data row3 col0\" >00:07</td>\n",
              "      <td id=\"T_59c8c_row3_col1\" class=\"data row3 col1\" >\"</td>\n",
              "      <td id=\"T_59c8c_row3_col2\" class=\"data row3 col2\" >We want to take you to the heart of where these ideas are coming from. We want to introduce you to the people who are leading the design of our collective future.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_59c8c_row4_col0\" class=\"data row4 col0\" >00:19</td>\n",
              "      <td id=\"T_59c8c_row4_col1\" class=\"data row4 col1\" >Anca Dragan</td>\n",
              "      <td id=\"T_59c8c_row4_col2\" class=\"data row4 col2\" >Getting the safety right is probably, I'd say, one of the most important challenges of our time. I want safe and capable.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_59c8c_row5_col0\" class=\"data row5 col0\" >00:27</td>\n",
              "      <td id=\"T_59c8c_row5_col1\" class=\"data row5 col1\" >\"</td>\n",
              "      <td id=\"T_59c8c_row5_col2\" class=\"data row5 col2\" >I want a bridge that will not collapse.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_59c8c_row6_col0\" class=\"data row6 col0\" >00:30</td>\n",
              "      <td id=\"T_59c8c_row6_col1\" class=\"data row6 col1\" >Pushmeet Kohli</td>\n",
              "      <td id=\"T_59c8c_row6_col2\" class=\"data row6 col2\" >Just give these scientists a superpower that they had not imagined earlier.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_59c8c_row7_col0\" class=\"data row7 col0\" >00:34</td>\n",
              "      <td id=\"T_59c8c_row7_col1\" class=\"data row7 col1\" >Jeff Dean</td>\n",
              "      <td id=\"T_59c8c_row7_col2\" class=\"data row7 col2\" >autonomous vehicles. It's hard to fathom that when you're working on a search engine.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_59c8c_row8_col0\" class=\"data row8 col0\" >00:38</td>\n",
              "      <td id=\"T_59c8c_row8_col1\" class=\"data row8 col1\" >Douglas Eck</td>\n",
              "      <td id=\"T_59c8c_row8_col2\" class=\"data row8 col2\" >We may see entirely new genre or entirely new forms of art come up. There may be a new word that is not music, painting, photography, movie making, and that AI will have helped us create it.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_59c8c_row9_col0\" class=\"data row9 col0\" >00:48</td>\n",
              "      <td id=\"T_59c8c_row9_col1\" class=\"data row9 col1\" >Professor Hannah Fry</td>\n",
              "      <td id=\"T_59c8c_row9_col2\" class=\"data row9 col2\" >You really want AGI to be able to peer into the mysteries of the universe.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_59c8c_row10_col0\" class=\"data row10 col0\" >00:51</td>\n",
              "      <td id=\"T_59c8c_row10_col1\" class=\"data row10 col1\" >Demis Hassabis</td>\n",
              "      <td id=\"T_59c8c_row10_col2\" class=\"data row10 col2\" >Yes, quantum mechanics, string theory, well, and the nature of reality.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_59c8c_row11_col0\" class=\"data row11 col0\" >00:55</td>\n",
              "      <td id=\"T_59c8c_row11_col1\" class=\"data row11 col1\" >Professor Hannah Fry</td>\n",
              "      <td id=\"T_59c8c_row11_col2\" class=\"data row11 col2\" >Ow.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_59c8c_row12_col0\" class=\"data row12 col0\" >00:57</td>\n",
              "      <td id=\"T_59c8c_row12_col1\" class=\"data row12 col1\" >Douglas Eck</td>\n",
              "      <td id=\"T_59c8c_row12_col2\" class=\"data row12 col2\" >the magic of AI.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "transcribe_video(TestVideo.GDM_PODCAST_TRAILER_PT59S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "josZMk6UqoN5"
      },
      "source": [
        "### ðŸŽ¬ Narrator-only video\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyCvdvAMhj39"
      },
      "source": [
        "This video is a documentary that takes viewers on a virtual tour of the Gombe National Park in Tanzania. There's no visible speaker. Jane Goodall is correctly detected as the narrator, her name is extracted from the credits:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "9a0squK0qoN5",
        "outputId": "f3a99d11-f237-4a38-e4c3-6e010a97b2f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Video ([source](https://storage.googleapis.com/cloud-samples-data/video/JaneGoodall.mp4))"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video src=\"https://storage.googleapis.com/cloud-samples-data/video/JaneGoodall.mp4\" controls  width=\"600\" >\n",
              "      Your browser does not support the <code>video</code> element.\n",
              "    </video>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google AI Studio API: Only YouTube URLs are currently supported\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Speakers (0)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_a50ce  {\n",
              "  color: #202124;\n",
              "  background-color: #BDC1C6;\n",
              "  border: 0;\n",
              "  border-radius: 0.5rem;\n",
              "  border-spacing: 0px;\n",
              "  outline: 0.5rem solid #BDC1C6;\n",
              "  margin: 1rem 0.5rem;\n",
              "}\n",
              "#T_a50ce th {\n",
              "  background-color: #E8EAED;\n",
              "}\n",
              "#T_a50ce th {\n",
              "  text-align: left;\n",
              "  padding: 0.25rem 1rem;\n",
              "}\n",
              "#T_a50ce td {\n",
              "  text-align: left;\n",
              "  padding: 0.25rem 1rem;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_a50ce\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_a50ce_level0_col0\" class=\"col_heading level0 col0\" >voice</th>\n",
              "      <th id=\"T_a50ce_level0_col1\" class=\"col_heading level0 col1\" >name</th>\n",
              "      <th id=\"T_a50ce_level0_col2\" class=\"col_heading level0 col2\" >company</th>\n",
              "      <th id=\"T_a50ce_level0_col3\" class=\"col_heading level0 col3\" >position</th>\n",
              "      <th id=\"T_a50ce_level0_col4\" class=\"col_heading level0 col4\" >role_in_video</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Transcripts (0)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_5d984  {\n",
              "  color: #202124;\n",
              "  background-color: #BDC1C6;\n",
              "  border: 0;\n",
              "  border-radius: 0.5rem;\n",
              "  border-spacing: 0px;\n",
              "  outline: 0.5rem solid #BDC1C6;\n",
              "  margin: 1rem 0.5rem;\n",
              "}\n",
              "#T_5d984 th {\n",
              "  background-color: #E8EAED;\n",
              "}\n",
              "#T_5d984 th {\n",
              "  text-align: left;\n",
              "  padding: 0.25rem 1rem;\n",
              "}\n",
              "#T_5d984 td {\n",
              "  text-align: left;\n",
              "  padding: 0.25rem 1rem;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_5d984\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_5d984_level0_col0\" class=\"col_heading level0 col0\" >start</th>\n",
              "      <th id=\"T_5d984_level0_col1\" class=\"col_heading level0 col1\" >speaker</th>\n",
              "      <th id=\"T_5d984_level0_col2\" class=\"col_heading level0 col2\" >transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "transcribe_video(TestVideo.JANE_GOODALL_PT2M42S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceCW9orEhj39"
      },
      "source": [
        "> ðŸ’¡ Over the past few years, I have regularly used this video to test specialized ML models and it consistently resulted in various types of errors. Gemini's transcription, including punctuation, is perfect.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ_xX8DHqoN5"
      },
      "source": [
        "### ðŸŽ¬ French video\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMSycFsdhj39"
      },
      "source": [
        "This French reportage combines on-the-ground footage of a specialized team that uses trained dogs to detect leaks in underground drinking water pipes. The recording takes place entirely outdoors in a rural setting. The interviewed workers are introduced with on-screen text overlays. The audio, captured live on location, includes ambient noise. There are also some off-screen or unidentified speakers. This video is rather complex. The multimodal transcription provides excellent results with no false positives:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYd_6Am0qoN5"
      },
      "outputs": [],
      "source": [
        "transcribe_video(TestVideo.BRUT_FR_DOGS_WATER_LEAK_PT8M28S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Obb6-g_phj39"
      },
      "source": [
        "> ðŸ’¡ Our prompt was crafted and tested with English videos, but works without modification with this French video. It should also work for videos in these [100+ different languages](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#languages-gemini).\n",
        "\n",
        "> ðŸ’¡ In a multilingual solution, we might ask to translate our transcriptions into any of those 100+ languages and even perform text cleanup. This can be done in a second request, as the multimodal transcription is complex enough by itself.\n",
        "\n",
        "> ðŸ’¡ Gemini's audio tokenizer detects more than speech. If you try to list non-speech sounds on audio tracks only (to ensure the response doesn't benefit from any visual cues), you'll see it can detect sounds such as \"dog bark\", \"music\", \"sound effect\", \"footsteps\", \"laughter\", \"applause\"â€¦\n",
        "\n",
        "> ðŸ’¡ In our data visualization tables, colored rows are inference positives (speakers identified by the model), while gray rows correspond to negatives (unidentified speakers). This makes it easier to understand the results. As the prompt we crafted favors accuracy over recall, colored rows are generally correct, and gray rows correspond either to unnamed/unidentifiable speakers (true negatives) or to speakers that should have been identified (false negatives).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeC7dVBsqoN5"
      },
      "source": [
        "### ðŸŽ¬ Complex video\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDSSSGIphj3-"
      },
      "source": [
        "This Google DeepMind video is quite complex:\n",
        "\n",
        "- It is highly edited and very dynamic\n",
        "- Speakers are often off-screen and other people can be visible instead\n",
        "- The researchers are often in groups and it's not always obvious who's speaking\n",
        "- Some video shots were taken 2 years apart: the same speakers can sound and look different!\n",
        "\n",
        "Gemini 2.0 Flash generates an excellent transcription despite the complexity. However, it is likely to list duplicate speakers due to the video type. Gemini 2.5 Pro provides a deeper inference and manages to consolidate the speakers:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64-dl-B7ln_r"
      },
      "outputs": [],
      "source": [
        "transcribe_video(\n",
        "    TestVideo.GDM_ALPHAFOLD_PT7M54S,\n",
        "    model=Model.GEMINI_2_5_PRO,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFaWO4vnqoN5"
      },
      "source": [
        "### ðŸŽ¬ Long transcription\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EgD258nln_r"
      },
      "source": [
        "The total length of the transcribed text can quickly reach the maximum number of output tokens. With our current JSON response schema, we can reach 8,192 output tokens (supported by Gemini 2.0) with transcriptions of ~25min videos. Gemini 2.5 models support up to 65,536 output tokens (8x more) and let us transcribe longer videos.\n",
        "\n",
        "For this 54-minute panel discussion, Gemini 2.5 Pro uses only ~30-35% of the input/output token limits:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTksIBcvln_r"
      },
      "outputs": [],
      "source": [
        "transcribe_video(\n",
        "    TestVideo.GDM_AI_FOR_SCIENCE_FRONTIER_PT54M23S,\n",
        "    model=Model.GEMINI_2_5_PRO,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nyN608L1EAz"
      },
      "source": [
        "> ðŸ’¡ In this long video, the five panelists are correctly transcribed, diarized, and identified. In the second half of the video, unseen attendees ask questions to the panel. They are correctly identified as audience members and, though their names and companies are never written on the screen, Gemini correctly extracts and even consolidates the information from the audio cues.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z23MVFlAhdvj"
      },
      "source": [
        "### ðŸŽ¬ 1h+ video\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "citR2M-C-CF5"
      },
      "source": [
        "In the latest Google I/O keynote video (1h 10min):\n",
        "\n",
        "- ~30-35% of the token limit is used (383k/1M in, 20/64k out)\n",
        "- The dozen speakers are nicely identified, including the demo \"AI Voices\" (\"Gemini\" and \"Casey\")\n",
        "- Speaker names are extracted from slanted text on the background screen for the live keynote speakers (e.g., Josh Woodward at 0:07) and from lower-third on-screen text in the DolphinGemma reportage (e.g., Dr. Denise Herzing at 1:05:28)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZoTbx-lhdvj"
      },
      "outputs": [],
      "source": [
        "transcribe_video(\n",
        "    TestVideo.GOOGLE_IO_DEV_KEYNOTE_PT1H10M03S,\n",
        "    model=Model.GEMINI_2_5_PRO,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg3f5Wcrhdvj"
      },
      "source": [
        "### ðŸŽ¬ 40 speaker video\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0w3xU-p-CF5"
      },
      "source": [
        "In this 1h 40min Google Cloud Next keynote video:\n",
        "\n",
        "- ~50-70% of the token limit is used (547k/1M in, 45/64k out)\n",
        "- 40 distinct voices are diarized\n",
        "- 29 speakers are identified, connected to their 21 respective companies or divisions\n",
        "- The transcription takes up to 8 minutes (approximately 4 minutes with video tokens cached), which is 13 to 23 times faster than watching the entire video without pauses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLOUQVIJhdvj"
      },
      "outputs": [],
      "source": [
        "transcribe_video(\n",
        "    TestVideo.GOOGLE_CLOUD_NEXT_PT1H40M03S,\n",
        "    model=Model.GEMINI_2_5_PRO,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymRS4G5yqoN5"
      },
      "source": [
        "### ðŸŽ¬ Transcribe your videos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fK9OcMCqoN5"
      },
      "outputs": [],
      "source": [
        "class MyVideo(Video):\n",
        "    # Templates for supported video sources\n",
        "    # For testing purposes, video duration is statically specified in the enum name\n",
        "    # Examples: MY_VIDEO_PT42S, MY_VIDEO_PT4M56S, MY_VIDEO_PT1H23M45S\n",
        "    A_PTxHyMzS = url_for_youtube_id(\"\")\n",
        "    B_PTxHyMzS = \"gs://bucket/path/to/video.*\"  # Vertex AI only\n",
        "    C_PTxHyMzS = \"https://path/to/video.*\"  # Vertex AI only\n",
        "\n",
        "    # Add your own videos\n",
        "\n",
        "\n",
        "# For quick tests, specify a VideoSegment (which will specify start and end offsets in the video metadata config):\n",
        "# transcribe_video(\n",
        "#     MyVideo.ABC,\n",
        "#     VideoSegment(start=timedelta(minutes=0), end=timedelta(minutes=5)),\n",
        "#     model=Model.GEMINI_2_0_FLASH,\n",
        "# )\n",
        "\n",
        "# For complex or 1h+ videos:\n",
        "# transcribe_video(\n",
        "#     MyVideo.ABC,\n",
        "#     model=Model.GEMINI_2_5_PRO,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tyq90-4hj38"
      },
      "source": [
        "---\n",
        "\n",
        "## âš–ï¸ Strengths & weaknesses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBf2ywEu-CF6"
      },
      "source": [
        "### ðŸ‘ Strengths\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okiqeIMk-CF6"
      },
      "source": [
        "Overall, Gemini is capable of generating excellent transcriptions that surpass human-generated ones in these aspects:\n",
        "\n",
        "- Consistency of the transcription\n",
        "- Perfect grammar and punctuation\n",
        "- Impressive semantic understanding\n",
        "- No typos or transcription system mistakes\n",
        "- Exhaustivity (every audible word is transcribed)\n",
        "\n",
        "> ðŸ’¡ As you know, a single incorrect/missing word (or even letter) can completely change the meaning. These strengths help ensure high-quality transcriptions and reduce the risk of misunderstandings.\n",
        "\n",
        "If we compare YouTube's user-provided transcriptions (sometimes by professional caption vendors) to our auto-generated ones, we can observe some significant differences. Here are some examples from the last test:\n",
        "\n",
        "| timecode | âŒ user-provided                        | âœ… our transcription                            |\n",
        "| -------: | --------------------------------------- | ----------------------------------------------- |\n",
        "|     9:47 | research and **models**                 | research and **model**                          |\n",
        "|    13:32 | used **by 100,000** businesses          | used **by over 100,000** businesses             |\n",
        "|    18:19 | infrastructure core **layer**           | infrastructure core **for AI**                  |\n",
        "|    20:21 | hardware **system**                     | hardware **generation**                         |\n",
        "|    23:42 | **I do** deployed ML models             | **Toyota** deployed ML models                   |\n",
        "|    34:17 | Vertex **video**                        | Vertex **Media**                                |\n",
        "|    41:11 | speed up **app** development            | speed up **application coding and** development |\n",
        "|    42:15 | performance **and proven** insights     | performance **improvement** insights            |\n",
        "|    50:20 | across the **milt** agent ecosystem     | across the **multi-agent** ecosystem            |\n",
        "|    52:50 | Salesforce, **and** Dun                 | Salesforce, **or** Dun                          |\n",
        "|  1:22:28 | please **almost**                       | Please **welcome**                              |\n",
        "|  1:31:07 | organizations, **like I say Charles**   | organizations **like Charles**                  |\n",
        "|  1:33:23 | multiple public **LOMs**                | multiple public **LLMs**                        |\n",
        "|  1:33:54 | Gemini's **Agent tech** AI              | Gemini's **agentic** AI                         |\n",
        "|  1:34:24 | mitigated **outsider** risk             | mitigated **insider** risk                      |\n",
        "|  1:35:58 | from **end point**, **viral**, networks | from **endpoint**, **firewall**, networks       |\n",
        "|  1:38:45 | We at **Google** are                    | We at **Google Cloud** are                      |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3Ey-xra-CF6"
      },
      "source": [
        "### ðŸ‘Ž Weaknesses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoF5HIth-CF7"
      },
      "source": [
        "The current prompt is not perfect though. It focuses first on the audio for transcription and then on all cues for speaker data extraction. Though Gemini natively ensures a very high consolidation from the context, the prompt can show these side effects:\n",
        "\n",
        "- Sensitivity to speakers' pronunciation or accent\n",
        "- Misspellings for proper nouns\n",
        "- Inconsistencies between transcription and perfectly identified speaker name\n",
        "\n",
        "Here are examples from the same test:\n",
        "\n",
        "| timecode | âœ… user-provided  | âŒ our transcription |\n",
        "| -------: | ----------------- | -------------------- |\n",
        "|     3:31 | Bosun             | Boson                |\n",
        "|     3:52 | Imagen            | Imagine              |\n",
        "|     3:52 | Veo               | VO                   |\n",
        "|    11:15 | Berman            | Burman               |\n",
        "|    25:06 | Huang             | Wang                 |\n",
        "|    38:58 | Allegiant Stadium | Allegiance Stadium   |\n",
        "|  1:29:07 | Snyk              | Sneak                |\n",
        "\n",
        "We'll stop our exploration here and leave it as an exercise, but here are possible ways to fix these errors, in order of simplicity/cost:\n",
        "\n",
        "- Update the prompt to use visual cues for proper nouns, such as _\"Ensure all proper nouns (people, companies, products, etc.) are spelled correctly and consistently. Prioritize on-screen text for reference.\"_\n",
        "- Enrich the prompt with an additional preliminary table to extract the proper nouns and use them explicitly in the context\n",
        "- Add available video context metadata in the prompt\n",
        "- Split the prompt into two successive requests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz9xIZL6-CF7"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ“ˆ Tips & optimizations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx0m5x62hdvk"
      },
      "source": [
        "### ðŸ”§ Model selection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSPZUxeQhdvk"
      },
      "source": [
        "Each model can differ in terms of performance, speed, and cost.\n",
        "\n",
        "Here's a practical summary based on the model specifications, our video test suite, and the current prompt:\n",
        "\n",
        "| Model            | Performance | Speed  |  Cost  | Max. input tokens | Max. output tokens | Video type                  |\n",
        "| ---------------- | :---------: | :----: | :----: | ----------------: | -----------------: | --------------------------- |\n",
        "| Gemini 2.0 Flash |    â­â­     | â­â­â­ | â­â­â­ |    1,048,576 = 1M |         8,192 = 8k | Standard video, up to 25min |\n",
        "| Gemini 2.5 Flash |    â­â­     |  â­â­  |  â­â­  |    1,048,576 = 1M |       65,536 = 64k | Standard video, 25min+      |\n",
        "| Gemini 2.5 Pro   |   â­â­â­    |   â­   |   â­   |    1,048,576 = 1M |       65,536 = 64k | Complex video or 1h+ video  |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGfKnWAwhj38"
      },
      "source": [
        "### ðŸ”§ Video segment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovwE6EXahj38"
      },
      "source": [
        "You don't always need to analyze videos from start to finish. You can indicate a video segment with start and/or end offsets in the [VideoMetadata](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/rpc/google.cloud.aiplatform.v1#videometadata) structure.\n",
        "\n",
        "In this example, Gemini will only analyze the 30:00-50:00 segment of the video:\n",
        "\n",
        "```python\n",
        "video_metadata = VideoMetadata(\n",
        "    start_offset=\"1800.0s\",\n",
        "    end_offset=\"3000.0s\",\n",
        "    â€¦\n",
        ")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx8Yw-4Qhj38"
      },
      "source": [
        "### ðŸ”§ Media resolution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvSo6URZhj38"
      },
      "source": [
        "In our test suite, the videos are fairly standard. We got excellent results by using a \"low\" media resolution (\"medium\" being the default), specified with the `GenerateContentConfig.media_resolution` parameter.\n",
        "\n",
        "> ðŸ’¡ This provides faster and cheaper inferences, while also enabling the analysis of 3x longer videos.\n",
        "\n",
        "We used a simple heuristic based on video duration, but you might want to make it dynamic on a per-video basis:\n",
        "\n",
        "```python\n",
        "def get_media_resolution_for_video(video: Video) -> MediaResolution | None:\n",
        "    if not (video_duration := get_video_duration(video)):\n",
        "        return None  # Default\n",
        "\n",
        "    # For testing purposes, this is based on video duration, as our short videos tend to be more detailed\n",
        "    less_than_five_minutes = video_duration < timedelta(minutes=5)\n",
        "    if less_than_five_minutes:\n",
        "        media_resolution = MediaResolution.MEDIA_RESOLUTION_MEDIUM\n",
        "    else:\n",
        "        media_resolution = MediaResolution.MEDIA_RESOLUTION_LOW\n",
        "\n",
        "    return media_resolution\n",
        "```\n",
        "\n",
        "> âš ï¸ If you select a \"low\" media resolution and experience an apparent loss of understanding, you might be losing important details in the sampled video frames. This is easy to fix: switch back to the default media resolution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq3xWHSdhj38"
      },
      "source": [
        "### ðŸ”§ Sampling frame rate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdYdSjiihj38"
      },
      "source": [
        "The default sampling frame rate of 1 FPS worked fine in our tests. You might want to customize it for each video:\n",
        "\n",
        "```python\n",
        "SamplingFrameRate = float\n",
        "\n",
        "def get_sampling_frame_rate_for_video(video: Video) -> SamplingFrameRate | None:\n",
        "    sampling_frame_rate = None  # Default (1 FPS for current models)\n",
        "\n",
        "    # [Optional] Define a custom FPS: 0.0 < sampling_frame_rate <= 24.0\n",
        "\n",
        "    return sampling_frame_rate\n",
        "```\n",
        "\n",
        "> ðŸ’¡ You can mix the parameters. In this extreme example, assuming the input video has a 24fps frame rate, all frames will be sampled for a 10s segment:\n",
        "\n",
        "```python\n",
        "video_metadata = VideoMetadata(\n",
        "    start_offset=\"42.0s\",\n",
        "    end_offset=\"52.0s\",\n",
        "    fps=24.0,\n",
        ")\n",
        "```\n",
        "\n",
        "> âš ï¸ If you use a higher sampling rate, this multiplies the number of frames (and tokens) accordingly, increasing latency and cost. As `10s Ã— 24fps = 240 frames = 4Ã—60s Ã— 1fps`, this 10-second analysis at 24 FPS is equivalent to a 4-minute default analysis at 1 FPS.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTzErz6Ahj38"
      },
      "source": [
        "### ðŸŽ¯ Precision vs recall\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xdMa78Lhj38"
      },
      "source": [
        "The prompt can influence the precision and recall of our data extractions, especially when using explicit versus implicit wording. If you want more qualitative results, favor precision using explicit wording; if you want more quantitative results, favor recall using implicit wording:\n",
        "\n",
        "| wording  | favors    | generates less  | LLM behavior                                                                   |\n",
        "| -------- | --------- | --------------- | ------------------------------------------------------------------------------ |\n",
        "| explicit | precision | false positives | relies more (or only) on the provided context                                  |\n",
        "| implicit | recall    | false negatives | relies on the overall context, infers more, and can use its training knowledge |\n",
        "\n",
        "Here are examples that can lead to subtly different results:\n",
        "\n",
        "| wording  | verbs                | qualifiers                                   |\n",
        "| -------- | -------------------- | -------------------------------------------- |\n",
        "| explicit | \"extract\", \"quote\"   | \"stated\", \"direct\", \"exact\", \"verbatim\"      |\n",
        "| implicit | \"identify\", \"deduce\" | \"found\", \"indirect\", \"possible\", \"potential\" |\n",
        "\n",
        "> ðŸ’¡ Different models can also behave differently for the same prompt. In particular, more performant models might seem more \"confident\" and make more implicit inferences or consolidations.\n",
        "\n",
        "> ðŸ’¡ As an example, in this [AlphaFold video](https://youtu.be/gg7WjuFs8F4?t=297), at the 04:57 timecode, \"Spring 2020\" is first displayed as context. Then, a short declaration from \"The Prime Minister\" is heard in the background (\"You must stay at home\") without any other hints. When asked to \"identify\" (rather than \"extract\") the speaker, Gemini is likely to infer more and attribute the voice to \"Boris Johnson\". There's absolutely no explicit mention of Boris Johnson; his identity is correctly inferred from the context (\"UK\", \"Spring 2020\", and \"The Prime Minister\").\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6IMWexM1EAr"
      },
      "source": [
        "### ðŸ·ï¸ Metadata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q8Eqps61EAr"
      },
      "source": [
        "In our current tests, Gemini only uses audio and frame tokens, tokenized from sources on Google Cloud Storage or YouTube. If you have additional video metadata, this can be a goldmine; try to add it to your prompt and enrich the video context for better results upfront.\n",
        "\n",
        "Potentially helpful metadata:\n",
        "\n",
        "- Video description: This can provide a better understanding of where and when the video was shot.\n",
        "- Speaker info: This can help auto-correct names that are only heard and not obvious to spell.\n",
        "- Entity info: Overall, this can help get better transcriptions for custom or private data.\n",
        "\n",
        "> ðŸ’¡ For YouTube videos, no additional metadata or transcript is fetched. Gemini only receives the raw audio and video streams. You can check this yourself by comparing your results with YouTube's automatic captioning (no punctuation, audio only) or user-provided transcripts (cleaned up), when available.\n",
        "\n",
        "> ðŸ’¡ If you know your video concerns a team or a company, adding internal data in the context can help correct or complete the requested speaker names (provided there are no homonyms in the same context), companies, and job titles.\n",
        "\n",
        "> ðŸ’¡ In this [French reportage](https://youtu.be/U_yYkb-ureI?t=376), in the 06:16-06:31 video shot, there are two dogs: Arnold and Rio. \"Arnold\" is clearly audible, repeated three times, and correctly transcribed. \"Rio\" is called only once, audible for a fraction of a second in a noisy environment, and the audio transcription can vary. Providing the names of the whole team (owners & dogs, even if they are not all in the video) can help in transcribing this short name consistently.\n",
        "\n",
        "> ðŸ’¡ It should also be possible to ground the results with Google Search, Google Maps, or your own RAG system. See [Grounding overview](https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/overview).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5xUX4Xrhj38"
      },
      "source": [
        "### ðŸ”¬ Debugging & evidence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCV4Rz41hj38"
      },
      "source": [
        "Iterating through successive prompts and debugging LLM outputs can be challenging, especially when trying to understand the reasons for the results.\n",
        "\n",
        "It's possible to ask Gemini to provide evidence in the response. In our video transcription solution, we could request a timecoded \"evidence\" for each speaker's identified name, company, or role. This enables linking results to their sources, discovering and understanding unexpected insights, checking potential false positivesâ€¦\n",
        "\n",
        "> ðŸ’¡ In the tested videos, when trying to understand where the insights came from, requesting evidence yielded very insightful explanations, for example:\n",
        ">\n",
        "> - Person names could be extracted from various sources (video conference captions, badges, unseen participants introducing themselves when asking questions in a conference panelâ€¦)\n",
        "> - Company names could be found from text on uniforms, backpacks, vehiclesâ€¦\n",
        "\n",
        "> ðŸ’¡ In a document data extraction solution, we could request to provide an \"excerpt\" as evidence, including page number, chapter number, or any other relevant location information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jscy22A1hj38"
      },
      "source": [
        "### ðŸ˜ Verbose JSON\n",
        "\n",
        "The JSON format is currently the most common way to generate structured outputs with LLMs. However, JSON is a rather verbose data format, as field names are repeated for each object. For example, an output can look like the following, with many repeated underlying tokens:\n",
        "\n",
        "```jsonc\n",
        "{\n",
        "  \"task1_transcripts\": [\n",
        "    { \"start\": \"00:02\", \"text\": \"We'veâ€¦\", \"voice\": 1 },\n",
        "    { \"start\": \"00:07\", \"text\": \"But weâ€¦\", \"voice\": 1 }\n",
        "    // â€¦\n",
        "  ],\n",
        "  \"task2_speakers\": [\n",
        "    {\n",
        "      \"voice\": 1,\n",
        "      \"name\": \"John Moult\",\n",
        "      \"company\": \"University of Maryland\",\n",
        "      \"position\": \"Co-Founder, CASP\",\n",
        "      \"role_in_video\": \"Expert\"\n",
        "    },\n",
        "    // â€¦\n",
        "    {\n",
        "      \"voice\": 3,\n",
        "      \"name\": \"Demis Hassabis\",\n",
        "      \"company\": \"DeepMind\",\n",
        "      \"position\": \"Founder and CEO\",\n",
        "      \"role_in_video\": \"Team Leader\"\n",
        "    }\n",
        "    // â€¦\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "To optimize output size, an interesting possibility is to ask Gemini to generate an XML block containing a CSV for each of your tabular extractions. The field names are specified once in the header, and by using tab separators, for example, we can achieve more compact outputs like the following:\n",
        "\n",
        "```xml\n",
        "<TASK1_TRANSCRIPT_CSV>\n",
        "start  text     voice\n",
        "00:02  We'veâ€¦   1\n",
        "00:07  But weâ€¦  1\n",
        "â€¦\n",
        "</TASK1_TRANSCRIPT_CSV>\n",
        "<TASK2_SPEAKER_CSV>\n",
        "voice  name            company                 position          role_in_video\n",
        "1      John Moult      University of Maryland  Co-Founder, CASP  Expert\n",
        "â€¦\n",
        "3      Demis Hassabis  DeepMind                Founder and CEO   Team Leader\n",
        "â€¦\n",
        "</TASK2_SPEAKER_CSV>\n",
        "```\n",
        "\n",
        "> ðŸ’¡ Gemini excels at patterns and formats. Depending on your needs, feel free to experiment with JSON, XML, CSV, YAML, and any custom structured formats. It's likely that the industry will evolve to allow even more elaborate structured outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wSb4Zdihj39"
      },
      "source": [
        "### ðŸ¿ï¸ Context caching\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc3FzyLUhj39"
      },
      "source": [
        "Context caching optimizes the cost and the latency of repeated requests using the same base inputs.\n",
        "\n",
        "There are two ways requests can benefit from context caching:\n",
        "\n",
        "- **Implicit caching**: By default, upon the first request, input tokens are cached, to accelerate responses for subsequent requests with the same base inputs. This is fully automated and no code change is required.\n",
        "- **Explicit caching**: You place specific inputs into the cache and reuse this cached content as a base for your requests. This provides full control but requires managing the cache manually.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41HF84eshj39"
      },
      "source": [
        "Example of implicit caching:\n",
        "\n",
        "```python\n",
        "model_id = \"gemini-2.0-flash\"\n",
        "video_file_data = FileData(\n",
        "    file_uri=\"gs://bucket/path/to/my-video.mp4\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "video = Part(file_data=video_file_data)\n",
        "prompt_1 = \"List the people visible in the video.\"\n",
        "prompt_2 = \"Summarize what happens to John Smith.\"\n",
        "\n",
        "# âœ… Request A1: static data (video) placed first\n",
        "response = client.models.generate_content(\n",
        "    model=model_id,\n",
        "    contents=[video, prompt_1],\n",
        ")\n",
        "\n",
        "# âœ… Request A2: likely cache hit for the video tokens\n",
        "response = client.models.generate_content(\n",
        "    model=model_id,\n",
        "    contents=[video, prompt_2],\n",
        ")\n",
        "```\n",
        "\n",
        "> ðŸ’¡ Implicit caching can be disabled at the project level (see [data governance](https://cloud.google.com/vertex-ai/generative-ai/docs/data-governance#customer_data_retention_and_achieving_zero_data_retention)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZYDfxz1hj39"
      },
      "source": [
        "Implicit caching is prefix-based, so it only works if you put static data first and variable data last.\n",
        "\n",
        "Example of requests preventing implicit caching:\n",
        "\n",
        "```python\n",
        "# âŒ Request B1: variable input placed first\n",
        "response = client.models.generate_content(\n",
        "    model=model_id,\n",
        "    contents=[prompt_1, video],\n",
        ")\n",
        "\n",
        "# âŒ Request B2: no cache hit\n",
        "response = client.models.generate_content(\n",
        "    model=model_id,\n",
        "    contents=[prompt_2, video],\n",
        ")\n",
        "```\n",
        "\n",
        "> ðŸ’¡ This explains why the data-plus-instructions input order is preferred, for performance (not LLM-related) reasons.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwcqJtZ-hj39"
      },
      "source": [
        "Cost-wise, the input tokens retrieved with a cache hit benefit from a 75% discount in the following cases:\n",
        "\n",
        "- **Implicit caching**: With all Gemini models, cache hits are automatically discounted (without any control on the cache).\n",
        "- **Explicit caching**: With all Gemini models and supported models in Model Garden, you control your cached inputs and their lifespans to ensure cache hits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpd76xLlhj39"
      },
      "source": [
        "Example of explicit caching:\n",
        "\n",
        "```python\n",
        "from google.genai.types import (\n",
        "    Content,\n",
        "    CreateCachedContentConfig,\n",
        "    FileData,\n",
        "    GenerateContentConfig,\n",
        "    Part,\n",
        ")\n",
        "\n",
        "model_id = \"gemini-2.0-flash-001\"\n",
        "\n",
        "# Input video\n",
        "video_file_data = FileData(\n",
        "    file_uri=\"gs://cloud-samples-data/video/JaneGoodall.mp4\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "video_part = Part(file_data=video_file_data)\n",
        "video_contents = [Content(role=\"user\", parts=[video_part])]\n",
        "\n",
        "# Video explicitly put in cache, with time-to-live (TTL) before automatic deletion\n",
        "cached_content = client.caches.create(\n",
        "    model=model_id,\n",
        "    config=CreateCachedContentConfig(\n",
        "        ttl=\"1800s\",\n",
        "        display_name=\"video-cache\",\n",
        "        contents=video_contents,\n",
        "    ),\n",
        ")\n",
        "if cached_content.usage_metadata:\n",
        "    print(f\"Cached tokens: {cached_content.usage_metadata.total_token_count or 0:,}\")\n",
        "    # Cached tokens: 46,171\n",
        "    # âœ… Video tokens are cached (standard tokenization rate + storage cost for TTL duration)\n",
        "\n",
        "cache_config = GenerateContentConfig(cached_content=cached_content.name)\n",
        "\n",
        "# Request #1\n",
        "response = client.models.generate_content(\n",
        "    model=model_id,\n",
        "    contents=\"List the people mentioned in the video.\",\n",
        "    config=cache_config,\n",
        ")\n",
        "if response.usage_metadata:\n",
        "    print(f\"Input tokens : {response.usage_metadata.prompt_token_count or 0:,}\")\n",
        "    print(f\"Cached tokens: {response.usage_metadata.cached_content_token_count or 0:,}\")\n",
        "    # Input tokens : 46,178\n",
        "    # Cached tokens: 46,171\n",
        "    # âœ… Cache hit (75% discount)\n",
        "\n",
        "# Request #i (within the TTL period)\n",
        "# â€¦\n",
        "\n",
        "# Request #n (within the TTL period)\n",
        "response = client.models.generate_content(\n",
        "    model=model_id,\n",
        "    contents=\"List all the timecodes when Jane Goodall is mentioned.\",\n",
        "    config=cache_config,\n",
        ")\n",
        "if response.usage_metadata:\n",
        "    print(f\"Input tokens : {response.usage_metadata.prompt_token_count or 0:,}\")\n",
        "    print(f\"Cached tokens: {response.usage_metadata.cached_content_token_count or 0:,}\")\n",
        "    # Input tokens : 46,182\n",
        "    # Cached tokens: 46,171\n",
        "    # âœ… Cache hit (75% discount)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goWu_e99hj39"
      },
      "source": [
        "> ðŸ’¡ Explicit caching needs a specific model version (like `â€¦-001` in this example) to ensure the cache remains valid and is not affected by a model update.\n",
        "\n",
        "> â„¹ï¸ Learn more about [Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ased6OeIhj39"
      },
      "source": [
        "### â³ Batch prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ1tJrE9hj39"
      },
      "source": [
        "If you need to process a large volume of videos and don't need synchronous responses, you can use a single batch request and reduce your cost.\n",
        "\n",
        "> ðŸ’¡ Batch requests for Gemini models get a 50% discount compared to standard requests.\n",
        "\n",
        "> â„¹ï¸ Learn more about [Batch prediction](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#generative-ai-batch-text-python_genai_sdk).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp2NMQQpa1E8"
      },
      "source": [
        "### â™¾ï¸ To productionâ€¦ and beyond\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7b5PlyKa1E8"
      },
      "source": [
        "A few additional notes:\n",
        "\n",
        "- The current prompt is not perfect and can be improved. It has been preserved in its current state to illustrate its development starting with Gemini 2.0 Flash and a simple video test suite.\n",
        "- The Gemini 2.5 models are more capable and intrinsically provide a better video understanding. However, the current prompt has not been optimized for them. Writing optimal prompts for different models is another challenge.\n",
        "- If you test transcribing your own videos, especially different types of videos, you may run into new or specific issues. They can probably be addressed by enriching the prompt.\n",
        "- Future models will likely support more output features. This should allow for richer structured outputs and for simpler prompts.\n",
        "- As models keep learning, it's also possible that multimodal video transcription will become a one-liner prompt.\n",
        "- Gemini's image and audio tokenizers are truly impressive and enable many other use cases. To fully grasp the extent of the possibilities, you can run unit tests on images or audio files.\n",
        "- We constrained our challenge to using a single request, which can dilute the LLM's attention in such rich multimodal contexts. For optimal results in a large-scale solution, splitting the processing into two steps (i.e., requests) should help Gemini's attention focus even further. In the first step, we'd extract and diarize the audio stream only, which should result in the most precise speech-to-text transcription (maybe with more voice identifiers than actual speakers, but with a minimal number of false positives). In the second step, we'd reinject the transcription to focus on extracting and consolidating speaker data from the video frames. This would also be a solution to process very long videos, even those several hours in duration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFO_u1-8hj3-"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nubJgw0Ahj3-"
      },
      "source": [
        "Multimodal video transcription, which requires the complex synthesis of audio and visual data, is a true challenge for ML practitioners, without mainstream solutions. A traditional approach, involving an elaborate pipeline of specialized models, would be engineering-intensive without any guarantee of success. In contrast, Gemini proved to be a versatile toolbox for reaching a powerful and straightforward solution based on a single prompt:\n",
        "\n",
        "![multimodal video transcription solutions](https://storage.googleapis.com/github-repo/generative-ai/gemini/use-cases/video-analysis/multimodal_video_transcription/multimodal-video-transcription-solutions.gif)\n",
        "\n",
        "We managed to address this complex problem with the following techniques:\n",
        "\n",
        "- Prototyping with open prompts to develop intuition about Gemini's natural strengths\n",
        "- Taking into account how LLMs work under the hood\n",
        "- Crafting increasingly specific prompts using a tabular extraction strategy\n",
        "- Generating structured outputs to move towards production-ready code\n",
        "- Adding data visualization for easier interpretation of responses and smoother iterations\n",
        "- Adapting default parameters to optimize the results\n",
        "- Conducting more tests, iterating, and even enriching the extracted data\n",
        "\n",
        "These principles should apply to many other data extraction domains and allow you to solve your own complex problems. Have fun and happy solving!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huldfEHkhj3-"
      },
      "source": [
        "---\n",
        "\n",
        "## âž• More!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_-odeqEln_s"
      },
      "source": [
        "- Explore additional use cases in the [Vertex AI Prompt Gallery](https://console.cloud.google.com/vertex-ai/studio/prompt-gallery)\n",
        "- Stay updated by following the [Vertex AI Release Notes](https://cloud.google.com/vertex-ai/generative-ai/docs/release-notes)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "multimodal_video_transcription.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}